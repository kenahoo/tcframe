\chapter{Evaluation}

In order to evaluate the quality of the \aicat\ framework, several
aspects of the framework have been tested.  The three main areas of
testing are quality of classification, efficiency, and ease of use.
For testing the quality of classification and efficiency, performance
is measured on categorization tasks using several different data sets.

Section \ref{Corpora} describes the data sets used during testing.
Section \ref{Quality} presents various measurements of how accurately
the framework performs on these data sets, and Section
\ref{Efficiency} discusses the computational efficiency of the
framework.  Section \ref{Applications} discusses the ease of use of
the framework in different contexts.


\section{Corpora}
\label{Corpora}

During development and testing, several data sets, or ``corpora,''
were used for framework testing and application building.  Since the
framework will behave differently on different data sets, it is
important to understand the characteristics of each corpus.  For
instance, different feature selection and categorization algorithms
may scale differently in relation to the size of the training corpus,
both in terms of efficiency and accuracy.\cite{chakrabarti:98} Also,
the specifics of the categorization problem in each corpus may be more
amenable to one categorization technique or another.

The main data sets used for testing are listed in this section.
Unless otherwise noted below, a list of common English words from
\cite{salton:89} was used as a ``stoplist,'' or a set of terms to
completely ignore when processing documents.  This is a common technique from
Information Retrieval,\cite{XXX-manning} as it is assumed that these words
possess little or no information about the target categories, and that
they will only slow processing and add noise to the data.


\subsection{ApteMod}


The ApteMod version of the Reuters-21578 corpus has become a
standard benchmark corpus in evaluating Text Categorization
systems.\cite{yang:99} It is a collection of 10,788 documents from the
Reuters newswire service, partitioned into a training set with 7769
documents and a test set with 3019 documents.  The total size of the
corpus is about 43 MB.  It is available for download from
\url{http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html}.

The distribution of categories in the ApteMod corpus is highly skewed,
with 36.7\% of the documents in the most common category, and only
0.0185\% (2 documents) in each of the five least common categories.
In fact, the original data source is even more skewed---in creating
the corpus, any categories that did not contain at least one document
in the training set and one document in the test set were removed from
the corpus.\cite{yang:99}

In the ApteMod corpus, each document belongs to one or more
categories.  The average number of categories per document is 1.235,
and the average number of documents per category is about 148, or
about 1.37\% of the corpus.

\subsection{DrMath}

The DrMath corpus is a collection of 6,630 English-language messages
sent to the ``Ask Dr. Math'' question-and-answer service for
students.\cite{drmath} Each message belongs to one or more categories,
with category names indicating both math topic and grade level,
e.g. ``High School Geometry.''  The ontology is generally not
separable into two separate category sets for independent topic and
level categorizations, in part because many topic and level
combinations like ``Elementary School Calculus'' don't exist in the
category scheme.

The 26 MB corpus is divided into a training set with 5,304 documents
and a test set with 1,326 documents.  As with the ApteMod corpus, the
category distribution is skewed, with 13.2\% of the documents in the
most common category and only 0.0452\% (3 documents) in the least
common category.  The average number of categories per document is
1.534, and the average number of documents per category is about 107,
or 1.61\% of the corpus.

The corpus is not available for direct download, but interested
parties may contact the author for details.


\subsection{Signal G (sensitivity)}

The Signal G corpus consists of 122,919 financial announcement documents from
the Australian Stock Exchange issued between January 4 and December 29, 2000.
Each documents has been hand-categorized according to whether it indicates
``market sensitivity'' or not.  Every document is a member of either the
``sensitive'' or ``insensitive'' category---we can view this as two
categories that partition the corpus, or as a single category
``sensitive'' that some documents belong to and others don't.  The
documents are split into a training set of 81,814 documents and a test
set of 41,105 documents.

\subsection{Signal G (report type)}

\subsection{Reuters-CRC}
This is a collection of financial announcements gathered from the
Reuters financial service in the year 2000.  It represents all
announcements sent by the top 500 companies on the Australian Stock
Exchange (determined by trading volume) during 2000.  There are 27,874
documents, partitioned randomly into 18,568 training documents and
9,306 test documents (105 MB training, 52.5 MB test).  We have imposed
a categorization scheme on the data set by examining trends in related
training data, so that each document becomes a member of either the
category ``insensitive'' or ``sensitive'', similar to the SignalG data
set.  The corpus is the property of Capital Markets Cooperative
Research Centre and is not available for use by outside groups.


\section{Quality of Classification}
\label{Quality}

\begin{table}
\begin{tabular}{|r c c c c c c c|}
\hline
method & M$R$ & M$P$ & M$F_1$ & $\mu R$ & $\mu P$ & $\mu F_1$ & error \\
\hline
NB       & .3659 & .4969 & .3959 & .7238 & .8514 & .7824 & .00555 \\
SVM      & .0836 & .2932 & .1148 & .4726 & .8657 & .6114 & .00807 \\
kNN      & \\
Baseline & .0135 & .0142 & .0137 & .1645 & .1664 & .1654 & .02287 \\
\hline
\end{tabular}
\caption{Results of \aicat\ on Reuters corpus}
\end{table}

\begin{table}
\begin{tabular}{|r c c c c c c c|}
\hline
method & M$R$ & M$P$ & M$F_1$ & $\mu R$ & $\mu P$ & $\mu F_1$ & error \\
\hline
NB  & ? & ? & .3886 & .7688 & .8245 & .7056 & .00544 \\
SVM & ? & ? & .5251 & .8120 & .9137 & .8599 & .00365 \\
kNN & ? & ? & .5242 & .8339 & .8807 & .8567 & .00385 \\
\hline
\end{tabular}
\caption{Results from \cite{yang:99} on Reuters corpus}
\end{table}


\section{Efficiency}
\label{Efficiency}

\section{Applications}
\label{Applications}

\section{Dissemination}
XXX - perhaps eliminate this section

\subsection{Documentation}
\subsection{Community involvement}
