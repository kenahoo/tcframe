\chapter{Evaluation}

In order to evaluate the quality of the \aicat\ framework, several
aspects of the framework have been tested.  The three main areas of
testing are quality of classification, efficiency, and ease of use.


\section{Corpora}

During development and testing, several data sets, or ``corpora,''
were used for framework testing and application building.  Since the
framework will behave differently on different data sets, it is
important to understand the characteristics of each corpus.  For
instance, different feature selection and categorization algorithms
may scale differently in relation to the size of the training corpus,
both in terms of efficiency and accuracy.\cite{chakrabarti:98} Also,
the specifics of the categorization problem in each corpus may be more
amenable to one categorization technique or another.

The main data sets used for testing are listed in this section.
Unless otherwise noted below, a list of common English words from
\cite{salton:89} was used as a ``stoplist,'' or a set of terms to
completely exclude from processing.  This is a common technique from
Information Retrieval,\cite{XXX} as it is assumed that these words
possess little or no information about the target categories, and that
they will only slow processing and add noise to the data.


\subsection{ApteMod}


The ``ApteMod'' version of the Reuters-21578 corpus has become a
standard benchmark corpus in evaluating Text Categorization
systems.\cite{yang:99} It is a collection of 10,788 documents from the
Reuters newswire service, partitioned into a training set with 7769
documents and a test set with 3019 documents.  The total size of the
corpus is about 43 MB.  It is available for download from
\url{http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html}.

The distribution of categories in the ApteMod corpus is highly skewed,
with 3,964 documents in the most common category, and only 2 documents
in each of the five least common categories.  In fact, the original
data source is even more skewed---in creating the corpus, any
categories that did not contain at least one document in the training
set and one document in the test set were removed from the
corpus.\cite{yang:99}

In the ApteMod corpus, each document belongs to one or more
categories.  The average number of categories per document is 1.235,
and the average number of documents per category is 148.089.

\subsection{SignalG}
This corpus consists of 122,919 financial announcement documents from
the Australian Stock Exchange between January 4 and December 29, 2000.
The documents are hand-categorized according to whether they indicate
``market sensitivity'' or not.  Every document is a member of either the
``sensitive'' or ``insensitive'' category---we can view this as two
categories that partition the corpus, or as a single category
``sensitive'' that some documents belong to and others don't.  The
documents are split into a training set of 81,814 documents and a test
set of 41,105 documents.


\subsection{DrMath}
This is a smallish collection (26.5 MB) of English-language messages
sent to the ``Ask Dr. Math'' question-and-answer service for students
(\url{http://www.mathforum.org/dr.math/}).  It consists of messages
containing math questions, categorized by topic and grade level.  Each
category specifies both topic and grade level information (i.e. ``High
School Geometry''), so these categorization tasks are not generally
separable into a categorization by topic and a categorization by
level.  The corpus is divided into a training set with 5304 documents
and a test set with 1326 documents.  The corpus is not available for
direct download, but you may contact Ken Williams for details.


\subsection{Reuters-CRC}
This is a collection of financial announcements gathered from the
Reuters financial service in the year 2000.  It represents all
announcements sent by the top 500 companies on the Australian Stock
Exchange (determined by trading volume) during 2000.  There are 27,874
documents, partitioned randomly into 18,568 training documents and
9,306 test documents (105 MB training, 52.5 MB test).  We have imposed
a categorization scheme on the data set by examining trends in related
training data, so that each document becomes a member of either the
category ``insensitive'' or ``sensitive'', similar to the SignalG data
set.  The corpus is the property of Capital Markets Cooperative
Research Centre and is not available for use by outside groups.


\section{Quality of Classification}

\begin{table}
\begin{tabular}{|r c c c c c c c|}
\hline
method & M$R$ & M$P$ & M$F_1$ & $\mu R$ & $\mu P$ & $\mu F_1$ & error \\
\hline
NB       & .3659 & .4969 & .3959 & .7238 & .8514 & .7824 & .00555 \\
SVM      & .0836 & .2932 & .1148 & .4726 & .8657 & .6114 & .00807 \\
kNN      & \\
Baseline & .0135 & .0142 & .0137 & .1645 & .1664 & .1654 & .02287 \\
\hline
\end{tabular}
\caption{Results of \aicat\ on Reuters corpus}
\end{table}

\begin{table}
\begin{tabular}{|r c c c c c c c|}
\hline
method & M$R$ & M$P$ & M$F_1$ & $\mu R$ & $\mu P$ & $\mu F_1$ & error \\
\hline
NB  & ? & ? & .3886 & .7688 & .8245 & .7056 & .00544 \\
SVM & ? & ? & .5251 & .8120 & .9137 & .8599 & .00365 \\
kNN & ? & ? & .5242 & .8339 & .8807 & .8567 & .00385 \\
\hline
\end{tabular}
\caption{Results from \cite{yang:99} on Reuters corpus}
\end{table}


\section{Efficiency}

\section{Applications}


\section{Dissemination}
\subsection{Documentation}
\subsection{Community involvement}
