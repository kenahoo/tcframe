\chapter{Background: Text Categorization}
\label{background-tc}

This chapter gives an overview of Text Categorization's terminology,
methodology, and common contexts.  Section \ref{formal-defs} provides
formal definitions of the foundations of TC methods, and the terms
defined in this section will be used throughout the rest of this
thesis.  Section \ref{tc-process} introduces several aspects of TC
that an application developer or researcher may need to control in a
TC application or experiment.  Section \ref{machine-learning}
discusses three machine Learning techniques common in TC, and Section
\ref{measures} defines some typical ways of evaluating the performance
of a TC system.


\section{Formal Definitions}
\label{formal-defs}

The goal of automatic Text Categorization is to automatically produce
specialized functions that can process natural-language documents,
assigning zero or more user-defined labels to them based on their
content. \cite[p. 3]{sebastiani:02} \cite[ch. 16]{manning:99}
\cite[sec. 6.10]{mitchell:97} More formally, given a set of labels
(i.e., categories) $\cats = \{c_1, \ldots, c_{|\cats|}\}$ and a set of
previously unseen documents $\docs = \{d_1, d_2, \ldots \}$, a categorizer is a
function $\func$ that maps from $\docs$ to the set of all subsets of
$\cats$.  Figure \ref{sets} shows a simple diagram of this action.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\linewidth]{figures/sets.pdf}
\caption{The action of a categorizer on a set of documents}
\label{sets}
\end{center}
\end{figure}

In some applications, categorizers assign only a single label to each
document, so a categorizer is often a function that maps directly from
$\docs$ to $\cats$. \cite[p. 3]{sebastiani:02} Often an intermediate
function is useful for \term{soft} or \term{rank-based}
categorization, mapping from $\docs \times \cats$ to the set of real
numbers $\mathbb{R}$ in order to assign a score to each category $c_j$
for each document $d_i$. \cite[p. 4]{sebastiani:02} The scored
categories may then be presented to a human expert in decreasing
order, and the human may then make the final decision on the
document's category membership.  Alternatively, the system may make a
decision itself based on a threshold for category membership,
transforming the problem back into the \term{hard} categorization
shown in Figure \ref{sets}.\footnote{This is the internal approach
  taken by the \aicat\ framework---see the description of the
  \class{Hypothesis} class in Section \ref{framework-classes} for more
  details.}

The standard modern approach to creating new categorizer functions is
to build them using Machine Learning techniques from a set of training
documents $\train$. \cite[p. 2]{sebastiani:02} This is a set of
user-provided, pre-labeled documents that follows a category
distribution similar to the distribution of $\docs$, and whose contents
provide information about what sorts of documents should be mapped to
what sorts of categories.  Algorithms can then be developed that make
generalizations about the relationship between document content and
document category, encoding these generalizations in the learned function $\func$.



\section{The Text Categorization Process}
\label{tc-process}

In order to train a categorizer in the above manner, the user must
begin with a \emph{training corpus},
hereafter referred to as $\train$.  This is a set of documents which
are pre-labeled with categories that are considered to be fully
correct---typically, they have been manually assigned by a
\emph{domain expert}, i.e. a person who is familiar with the type of
material contained in the documents and who knows how to assign a set
of categories to each document based on the documents' content.

The basic outline for creating Text Categorization applications is relatively simple: the
documents in $\train$ are presented to the TC system, the system
process the documents' content, and a specific categorization
function $\func$ is produced that may be used to categorize future documents
from the set $\docs$.  In an application, however, many details of
this process need to be managed in specific and often varying ways.
Sections \ref{Document storage} through \ref{Hypothesis behavior}
describe the stages of this process.

\subsection{Document storage}
\label{Document storage}

In an organization that needs a TC application, documents may have
many different origins.  They may originate from plain-text or
formatted email messages, they may be raw or pre-processed web pages,
they may be collections of data from a database, or they may not have
a straightforward representation outside of the TC system at all.  It
is therefore important to recognize that the notion of varying
document storage media, and the process of converting from those media
to a medium accessible to the TC system, is an important part of the
TC process.

In addition, many Text Categorization data sets are quite large.  In
their raw format, they may commonly be larger than the amount of
available memory on the machine processing them.  This has two
important implications.  First, converting the documents to a special
storage format (for instance, as a set of files on the local
filesystem) so that the TC system can access them may be impossible or
undesirable for reasons of time, space, and/or data redundancy.
Second, a mechanism that can deal with iterating through the native
storage medium of the documents without reading all document data into
memory is probably necessary in a TC system.

\subsection{Document format}
\label{Document format}

Although most of the academic TC literature considers a document to be
a simple plain text string of data, this may rarely be the case in an
application environment.  Documents may be stored in many different
formats, including plain text, HTML, PDF, XML, Microsoft Word .doc
format, MIME-encoded email messages, and so on.  The internal data in
each document may also be considered part of its format when
nontrivial amounts of information extraction or other transformations
need to be applied to the document data in order to make it accessible
to the TC system.  For example, digit-strings in some document
collections may be useful as terms to consider when categorizing,
whereas in other collections they may only add noise to the data.

For reasons similar to those mentioned in the previous section, it may
be desirable for a TC system to deal with these issues directly, or
more likely, provide a mechanism for a developer to extend the system
in this way, rather than forcing the developer to convert all data to
a format recognized by the system.

\subsection{Document structure}

Separate from the issue of document \emph{format} is that of document
\emph{structure}.  In an age when XML data is increasingly more common
as a data exchange and storage format, the structure of a document,
i.e. the way the constituent parts of a document interrelate and nest
to create the entire document, may be important to understanding the
document's meaning.

In the TC literature, little is currently made of document structure,
except that a TC system may assign importance weights to the terms in
a document according to pre-set importance weights of the sections in
which those terms were found.  For instance, a term found in the title
of a document might be considered twice as important as a term found
in the body.  However, as research into categorization of structured
documents progresses, this may be an important area to consider in
building TC systems.

\subsection{Tokenizing of data}

The segmenting of text data into chunks representing individual words
may seem like a straightforward task, but in fact there are many
variations on how this process, called \emph{tokenizing} in this
context, can be performed. \cite[sec. 4.2.2]{manning:99} It is not
sufficient to split the data by using whitespace (spaces, tabs
returns, and the like) as delimiters, because this does not deal with
punctuation characters.  It is also not obvious whether terms with
punctuated suffixes like \emph{boy's} or \emph{doesn't}, or hyphenated terms
like \emph{ready-made} ought to be treated as one feature or multiple
features---this decision will usually need to be made with some
knowledge of the document set $\docs$.  In addition, many non-European
written languages such as Japanese or Korean do not contain spaces indicating divisions between
words, so a sophisticated tokenizer may be required when dealing with
languages like these.

In addition, the tokenization process (or equivalently, a stage
directly after tokenization) may remove from the data any term in a
pre-defined list of ``stop words,'' which are words that occur very
commonly in the domain (such as ``the'' or ``and'' in most English
texts) and are assumed to contain little or no relevance to the
categorization problem at hand. \cite[p. 15]{sebastiani:02}
\cite{mladenic:98}

In order to address these issues, a TC system needs to allow
variations in the tokenizing process.  This may involve providing a
set of parameters that the user can adjust in order to affect
tokenizing, or in some cases the application developer may need to
write custom code to handle domain-specific cases.

\subsection{Dimensionality reduction}
\label{dim-reduction}

Like many Language Processing research fields, much of Text Categorization has
to do with the problem of \term{high dimensionality}.
\cite[p. 13]{sebastiani:02} \cite{joachims:98} The
dimensionality of the space in which the Machine Learning algorithm
operates can be as large as the total number of distinct terms in
$\train$.

High dimensionality may present two problems.  First, the Machine Learning
algorithm may scale poorly with increased dimensionality, so that
the training or categorization phases of the system's operation may
require more time or memory than is practical. \cite{chakrabarti:98} Second, the
$\train$ data in a high-dimensional space may be too sparse, with
not enough nonzero data points to make any useful inductive leap
during training.  This is particularly true in some highly
morphological languages like Finnish, in which a single word stem may
have thousands or millions of inflected forms, and most forms may only
be seen one time in all of $\train$, making them almost useless for
inductive learning.

One way to address the problem of high dimensionality is by applying a
linguistic \emph{stemming} algorithm to the terms found in $\train$
and $\docs$.  These algorithms transform words by removing prefix and
suffix morphemes, so that words like \emph{running} and \emph{runner}
collapse to their linguistic stem \emph{run}.  Although the use of
such processing has occasionally been reported to harm overall system
performance \cite{baker:98}, availability of such an algorithm is
usually seen as a necessary component of a TC
system. \cite[p. 12]{sebastiani:02}

Another way to reduce dimensionality in a TC system is to apply
\emph{feature selection} and/or \emph{feature extraction}.  Both are
statistical techniques to transform the entire set of document terms
into a smaller feature set with less sparsity.  The former does this
by choosing the ``most relevant'' terms using some statistical
criterion to measure relevance.  The latter
applies some transformation such as singular value decomposition (used
in the ``Latent Semantic Indexing'' technique) or grouping terms into clusters in
order to create a new, lower-dimensional space of features.

Three of the most commonly used feature selection criteria are the
\emph{information gain} (IG), \emph{document frequency} (DF), and
\emph{$\chi^2$} metrics. \cite{yang:97} $\text{DF}(f_k)$ is simply the
number of documents of $\train$ in which the feature $f_k$ occurs.
$\chi^2$ is defined as follows---here, $A$, $B$, $C$, and $D$ are
defined as the terms in the contingency table shown in Table
\ref{termcat-contingency}.

\begin{align*}
\chi^2(f_k,c_j) & = \frac{|\train|(AD-CB)^2}{(A+C)(B+D)(A+B)(C+D)} \\
\end{align*}


\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\cline{3-4}
\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{$c_j$ \textbf{occurs}} \\
\cline{3-4}
\multicolumn{2}{c|}{} & Yes & No \\
\hline
$f_k$           & Yes & $A$ & $B$ \\
\cline{2-4}
\textbf{occurs} & No  & $C$ & $D$ \\
\hline
\end{tabular}
\end{center}
\caption[Contingency table for category $c_j$ and term $f_k$]
 {Contingency table for category $c_j$ and term $f_k$.  The
  quantities $A$-$D$ represent the number of documents with the given
  properties.}
\label{termcat-contingency}
\end{table}

In order to find the overall $\chi^2(f_k)$ metric, the terms
$\chi^2(f_k,c_j)$ may either be averaged (typically weighted by the
frequency of each category), or the maximum value for any category may
be adopted. \cite{yang:97}

$IG$ is defined as follows, where $P(f_k)$ and $P(\overline{f_k})$ are
the probabilities that a document does or does not contain $f_k$,
respectively, $\cats_{f_k}$ and $\cats_{\overline{f_k}}$ are the
category sets in the subsets of $\docs$ containing $f_k$ or not,
respectively, and $\Entr(x)$ is the standard entropy function from
Information Theory. \cite[ch. 2]{manning:99}

\begin{equation*}
        IG(f_k) = \Entr(\cats)
                  - P(f_k) \Entr(\cats_{f_k})
	          - P(\overline{f_k}) \Entr(\cats_{\overline{f_k}})
%        IG(f_k) = - \Entr(\cats) + P(f_k) \Entr(\cats|f_k)
%	          + P(\overline{f_k}) \Entr(\cats|\overline{f_k})
\end{equation*}


\subsection{Vector space modeling}

The discussion in the previous section suggests that each document may
be viewed as a vector in a global vector space whose dimensions
represent the set of all unique features from $\train$.  This idea
forms the basis for several Machine Learning techniques, including
Support Vector Machines and k-Nearest-Neighbor categorizers.  It also
allows for arbitrary vector processing algorithms on the document data
to improve categorization results.

A common set of algorithms used for this purpose is the \emph{TF/IDF}
term-weighting scheme of Salton and Buckley \cite{salton:88}, which
allows for several different ways to represent a document as a vector
in the global vector space.  Terms may be weighted by their frequency
in the document, by the logarithm of that frequency, or by a boolean
figure representing only the presence or absence of the term.  Term
weight may also be reduced by a factor representing prevalence in
other documents, and the overall length of the document vector may be
scaled in several different ways.  The TF/IDF vector weighting
techniques are used commonly in TC systems, and their availability is
desirable for the \aicat\ framework.

\subsection{Machine Learning algorithm}

Many different Machine Learning algorithms are actively studied in the
TC research literature, and new algorithms or variations on existing
algorithms are continually being developed.  In addition, the choice
of algorithm may depend on the specific application---algorithms
differ not only in their ability to perform accurately on differing
data sets, but also in the resources they may require during training
and when categorizing documents.  Therefore, it is not possible to
choose a single Machine Learning algorithm for incorporation into the
\aicat\ framework.  As a TC system, it needs to allow for selection
among several standard algorithms and for incorporation of novel
algorithms developed by researchers.

Section \ref{machine-learning} gives an overview of three well-studied
Machine Learning algorithms and compares some of their relevant
characteristics.  Section \ref{Strategy} in Chapter \ref{design} shows
how the architecture of \aicat\ allows for flexibility in this aspect
of categorization.

\subsection{Machine Learning configuration}
\label{ml-config}

Even within a single Machine Learning algorithm, there may be several
parameters that a supervisor may vary to influence the training and
categorization processes.  For instance, the k-Nearest-Neighbor
algorithm has an adjustable parameter $k$, the SVM algorithm allows
for several variations in the type of kernel used, and most
categorization algorithms admit some type of control to trade off
precision and recall against each other (see Section \ref{measures}
for an explanation of these terms).  In order to achieve the
appropriate performance for a given task, application developers need
simple ways to vary these parameters.

In fact, this issue is not unique to the Machine Learning component of
the TC process.  Several of the previously discussed aspects of the TC
task, including feature selection, dimensionality reduction, and
vector space transformation, may be controlled by parameters that the
supervisor may wish to vary.  Consistency in the system's handling of
parameters may therefore be an important part of its design.  This
issue will be discussed again in Section \ref{factory-method}.

\subsection{Incremental learning}

In some TC applications, it may be desirable to incorporate feedback
from the user about whether the system's categorization decisions have
been correct or incorrect. \cite[p. 28]{sebastiani:02} This may allow
for a relatively small initial training set $\train$, or for
categorization on concepts which may change over time.  This process
is called \emph{incremental} or \emph{on-line} learning.

Unfortunately, incremental learning is not possible with all Machine
Learning methods, since some algorithms (e.g. Neural Network
categorizers) may not be able to incorporate new evidence into their
model without going through the entire training process again.  For
those algorithms which can support it, however, the use of incremental
learning may be considered important in building a TC application, and
is therefore considered a goal of the \aicat\ project.

\subsection{Hypothesis behavior}
\label{Hypothesis behavior}

Most of the standard Text Categorization literature assumes that the
goal of TC is to assign each document to one of two mutually exclusive
categories, otherwise known as \emph{binary
categorization}. \cite[p. 3]{sebastiani:02} Of course, real-world
problems often involve ontologies that consist of more than two
categories, and membership may not be mutually exclusive.  For
instance, some applications may require assigning only the most
appropriate category from $\cats$, some may require assigning any
appropriate category, and some, such as rank-based tasks, may require
an appropriateness score linking each category-document pair.

This situation does not represent a theoretical disconnect between
research and practice, because each multi-category TC problem can be
re-posed as a series of binary problems.  Most application builders,
however, will not want to actually re-pose their problems in this
manner, because it requires extra work, and it may obscure the true
nature of the application under development.  Therefore, it is
desirable for a TC system to offer support for all the scenarios
described in the previous paragraph, without requiring the
implementers of the Machine Learning algorithms to explicitly code for
them.  The way in which this is achieved is discussed in Section
\ref{class-overview}.

\section{Machine Learning techniques}
\label{machine-learning}

This section describes three Machine Learning techniques that are
common for Text Categorization: \naive\ Bayes categorizers, Support
Vector Machines, and k-Nearest-Neighbor categorizers.

\subsection{\naive\ Bayes}

\naive Bayes categorizers are extremely well-represented in the TC
literature, with many papers published examining their theory and
performance \cite{lewis:98,yang:99,sebastiani:02}.
Their theory rests on Bayes' Theorem of conditional probability, shown
in Equation \ref{bayes}.  For those unfamiliar with conditional
probabilities, the notation $P(a|b)$ means ``the probability of $a$
given $b$.''

\begin{equation} \label{bayes}
P(x|y) = \frac{P(y|x) P(x)}{P(y)}
\end{equation}

The quantity of interest when determining the relevance of a
particular document $d_i$ to a category $c_j$ is $P(c_j|d_i)$.  Any
category with a high enough conditional probability will be considered
assigned to $d_i$.  In particular, the ``best'' category will be
$\ArgMax_{c_j\in\cats} P(c_j|d_i)$. This probability is usually impossible to
compute directly, however, because $d_i$ has likely never been
encountered before.  Therefore, Bayes' Theorem can be applied to
change the probabilistic expression to one whose terms may be
estimated from the training data $\train$ as follows.

\begin{align*}
\ArgMax_{c_j\in\cats} P(c_j|d_i) 
 & = \ArgMax_{c_j\in\cats} \frac{P(d_i|c_j) P(c_j)}{P(d_i)} & (\text{by (\ref{bayes})}) \\
 & = \ArgMax_{c_j\in\cats} P(d_i|c_j) P(c_j) & (P(d_i) \text{ is constant}) \\
\end{align*}

$P(c_j)$ may be easily estimated from the frequency with which
documents appear in $c_j$ in $\train$.  To estimate $P(d_i|c_j)$,
$d_i$ may be considered equivalent to the string of its features
$f_{i1} f_{i2} \ldots f_{ik}$.  $\ArgMax_{c_j\in\cats} P(c_j|d_i)$ may
then be estimated as follows.

\begin{align*}
\ArgMax_{c_j\in\cats} P(c_j|d_i) 
 & = \ArgMax_{c_j\in\cats} P(f_{i1} \ldots f_{ik}|c_j) P(c_j) \\
 & \approx \ArgMax_{c_j\in\cats} P(f_{i1}|c_j) \cdot \ldots \cdot P(f_{ik}|c_j) P(c_j) \\
\end{align*}

This final step, which gives this algorithm its ``na\"ive'' moniker,
involves two conditional independence assumptions: first, that the
features $f_{i1}, \ldots, f_{ik}$ are conditionally independent given
the category $c_j$, and second, that the position of features within
document $d_i$ has no effect on the probability.  These features may
not be true in general---features may in fact correlate in complex
ways in real-world documents.  Nevertheless, the \naive\ Bayes
categorizer tends to produce fairly good results, and an analysis of
this phenomenon can be found in \cite{domingos:97}.

The conditional probabilities $P(f_{i1}|c_j), \ldots, P(f_{ik}|c_j)$
are typically estimated by measuring the relative frequencies of the
features $f_{i1}, \ldots, f_{ik}$ in the documents belonging to
$c_j$.  For features found in the documents of $\docs$ which were not
encountered in $\train$, it would be inappropriate to use this
estimate, however, because it would yield a probability of zero and
render the rest of the terms useless.  For this reason, unknown terms
are typically assigned some small nonzero probability in a process
known as \emph{smoothing}.

The \naive\ Bayes algorithm is fairly fast and non-memory-intensive.
Because its training process consists merely of counting the features
of the training documents, its training time scales linearly with
$|\train|$.  Categorization is also fairly fast, because all the
pre-computed probabilities $P(f_{il}|c_j)$ may simply be looked up in
an array.  Categorization of a single document therefore scales
linearly with $|\cats|$.  Because the system need only store feature
information on a per-category basis instead of a per-document basis,
the size of the trained categorizer will stay fairly small compared to
more resource-intensive categorizers like k-Nearest-Neighbor.

\subsection{k-Nearest-Neighbor}
\label{knn}

The k-Nearest-Neighbor algorithm (kNN) is one of the most conceptually
simple TC algorithms in the literature.  All documents in $\train$ are
considered as vectors in a space with a similarity measure $m: \docs
\times \docs \to \mathbb{R}$.  To determine whether an unseen document
$d_i$ is assigned to a category $c_j$, the $k$ most similar documents
to $d_i$ using the measure $m$ are determined, where $k$ is a
user-adjustable parameter.  If the number of these $k$ documents that
belong to $c_j$ (possibly weighted by the similarity measure $m$ for
each similar document) is greater than some pre-defined threshold,
then $d_i$ is assigned to $c_j$, and otherwise not.  This technique
has been described in \cite[p. 28]{sebastiani:02}, \cite{yang:99}, and
\cite{yang:97}, among others.

The choices for the $k$ parameter, the category-membership threshold,
the similarity function $m$, and how to map from the similarity scores
$m$ to the overall score for $c_j$ provide for many variations on the
standard algorithm.  For instance, a single membership threshold may
be used for all categories, or a different threshold may be used for
each category, possibly learned by optimizing performance on a
validation set.  In addition, if more than one document is being
categorized in a batch operation, several differing strategies for
thresholding may be employed that take advantage of the overall
proportions of documents that belong to each category. \cite{yang:01}

Although the k-Nearest-Neighbor algorithm is conceptually simple, it
is computationally intensive.  Unless thresholds are learned from a
validation set, there is no actual training stage when building a
categorizer---all decisions are made by computations performed during
categorization.  The time to train a kNN categorizer is therefore
minimal or null, but the time to categorize a single unseen document
scales linearly with $|\train|$ and must be performed in full for each
categorization.  In addition, the entire training corpus $\train$ must
be preserved in the categorizer's model, so memory or storage
requirements may be prohibitively high in some environments.

\subsection{Support Vector Machines}

Support Vector Machines (SVM) are another extremely active area of
research in the Text Categorization literature.  Their use in TC was
introduced by \cite{joachims:98}, and several studies, including
\cite{joachims:98} and \cite{yang:99}, have found their results to be
highly competitive with other Machine Learning methods on the standard
benchmark corpora.

SVM techniques are similar to kNN in that they view the training
documents as vectors in a vector space, and that they require a
similarity function (called the ``kernel'' function) that plays a role
similar to the function $m$ in Section
\ref{knn}. \cite[ch. 1]{scholkopf:02} However, instead of considering
the documents most similar to the document to be categorized, SVM
algorithms learn a \emph{decision surface} during training which
divides the vector space into regions that indicate category
membership.  Categorization then simply consists of determining which
side of the decision surface each document to be categorized lies on.

One key advantage of SVMs is that they can deal well with very large
feature spaces, both in terms of the correctness of the categorization
results and the efficiency of the training and categorization
algorithms.  This implies that little or no feature selection may need
to be performed on the training data, removing a possibly
time-consuming aspect of the TC process.  Unfortunately, a
disadvantage of many SVM training algorithms is that they scale poorly
with $|\train|$, in some cases requiring as much as $O(|\train|^3)$ or
$O(|\train|^4)$.  This may make their use with large numbers of
documents prohibitive unless the standard algorithms are modified.


\section{Performance Measures}
\label{measures}

Several statistical measures have become standard in the area of
evaluating Text Categorization systems. \cite[p. 33]{sebastiani:02}
Some of the most prevalent are based on the notions of
\emph{precision} and \emph{recall} from the field of Information
Retrieval. \cite{rijsbergen:79} Precision, often denoted by the symbol
$\pi$, measures the probability that a document assigned by the TC
system to a given category actually belongs to that category.
Conversely, recall, denoted by $\rho$, measures the probability that a
document actually belonging to a certain category will be assigned
during testing to that category. \cite[p. 33]{sebastiani:02}

The probabilities mentioned above can be estimated during testing by
comparing how often the TC system's category choices match the correct
categories.  A valuable tool for this analysis is the ``contingency
table,'' which summarizes the results of the experiment for a given
category.  Table \ref{onecat-contingency} shows a contingency table
for the category $c_i$, i.e. any arbitrary category in the
categorization scheme of the corpus.  Here, $A_i$, etc. represent the
number of documents that fall into the given situation, i.e. $A_i$ is
the number of test documents assigned to category $c_i$ by both the
expert and the TC system.

This allows us to estimate $\pi$ and $\rho$, whose true values are
$P(Expert=Y | System=Y)$ and $P(System=Y | Expert=Y)$, respectively,
in terms of the entries in the contingency table.  Since the number of
documents assigned to category $c_i$ by the TC system is $A_i+B_i$,
and the number assigned by the expert is $A_i+C_i$, our estimates for
$\pi$ and $\rho$ are $\frac{A_i}{A_i + B_i}$ and $\frac{A_i}{A_i +
C_i}$, respectively.



\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\cline{3-4}
\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{\textbf{Expert choice}} \\
\cline{3-4}
\multicolumn{2}{c|}{} & Yes & No \\
\hline
\textbf{System} & Yes & $A_i$ & $B_i$ \\
\cline{2-4}
\textbf{choice} & No  & $C_i$ & $D_i$ \\
\hline
\end{tabular}
\end{center}
\caption{Contingency table for category $c_i$}
\label{onecat-contingency}
\end{table}


$\pi$ and $\rho$ give valuable information about the performance of a
TC system, but neither provides an isolated rating of the system's
quality.  The reason is that either measure can usually be improved in
a system to the detriment of the other. \cite[p. 35]{sebastiani:02} For
instance, the \emph{trivial acceptor} categorizer, which assigns every
document to every category, will have a perfect $\rho$ score of 1, but
its precision will be unacceptably low on any nontrivial task.

Therefore, a measure that combines $\pi$ and $\rho$ is desirable as an
overall measure of the quality of the TC system.  One such measure is
the $F_\beta$ measure, first introduced to the Information Retrieval
literature by van Rijsbergen \cite[ch. 7]{rijsbergen:79}.  It is
defined by the equation $F_\beta = \frac{(\beta^2 + 1)\pi\rho}{\beta^2
\pi + \rho}$, where $0 \leq \beta \leq \infty$.  The $\beta$ parameter
provides a continuous way to balance between the importance of $\pi$
and $\rho$, with values closer to 0 emphasizing $\pi$, values closer
to $\infty$ emphasizing $\rho$, and a value of 1 balancing the two
measures equally.  Without specific knowledge of an application's
requirements (for instance, whether false positives for a certain
category are more problematic than false negatives), one may presume
that $\pi$ and $\rho$ are equally important, and therefore the
literature often uses $F_1$ as a measure of the quality of a TC system
on a particular category.

${F_1}_i$ may be derived in terms of the entries of the per-category
contingency table as follows:

\begin{equation*}
\begin{split}
{F_1}_i 
 & = \frac{ 2 \pi_i \rho_i}{\pi_i + \rho_i} \\[6pt]
 & = \frac{ \frac{2 {A_i}^2}{(A_i+B_i)(A_i+C_i)} } { \frac{A_i}{A_i+B_i} + \frac{A_i}{A_i+C_i} } \\[6pt]
 & = \frac{ 2 {A_i}^2 }                            { A_i(A_i+C_i) + A_i(A_i+B_i) } \\[6pt]
 & = \frac{ 2 A_i }                                { 2 A_i + B_i + C_i } \\[6pt]
\end{split}
\end{equation*}

Two other measures of categorization quality, \emph{error} and
\emph{accuracy}, are also sometimes encountered in the TC literature.
These are simple measures which can also be defined in terms of the
contingency table in Table \ref{onecat-contingency}: $error =
\frac{B_i+C_i}{A_i+B_i+C_i+D_i)}$, and $accuracy =
\frac{A_i+D_i}{A_i+B_i+C_i+D_i)}$.  In other words, error is the
proportion of the system's decisions that matched the expert's
choices, and accuracy is the proportion that did not.  As summarized
in \cite[p. 34]{sebastiani:02}, these are not always useful measures
of categorization quality, because the \emph{trivial rejector} (a
system that never assigns any documents to any category) will often
have a lower error and higher accuracy than most nontrivial
categorizers.  Nonetheless, error will be measured for the evaluation
tasks here, because it may give insight into the character of the
system's performance.


\subsection{Combining Measures Across Categories}
\label{combining-measures}

Section \ref{measures} introduced several performance measures that
may be defined to evaluate a categorizer on a single category.  In
order to evaluate the categorizer's overall performance on the entire
set of test documents, it is desirable to combine the per-category
scores $\pi_i$, $\rho_i$, and ${F_1}_i$ into overall scores for the
entire category set.

Two methods for doing this are standard in the literature.  The first
is called \emph{micro-averaging}, and sums the terms in the
contingency table for all categories simultaneously rather than in
per-category tables.  In other words, the micro-averaged $\pi$,
$\rho$, and $F_1$, notated $\pi^\mu$, $\rho^\mu$, and $F^\mu_1$, are
defined in terms of the per-category contingency tables by the
following equations.

\begin{equation*}
 \pi^\mu = \frac{\sum_{i=1}^{|C|}{A_i}} {\sum_{i=1}^{|C|}{A_i+B_i}} \qquad
\rho^\mu = \frac{\sum_{i=1}^{|C|}{A_i}} {\sum_{i=1}^{|C|}{A_i+C_i}} \qquad
 F^\mu_1 = \frac{\sum_{i=1}^{|C|}{2 A_i}} {\sum_{i=1}^{|C|}{2 A_i+B_i+C_i}} \qquad
\end{equation*}

Micro-averaging gives equal weight to each categorization decision
made by the system, or equivalently, to each document in the corpus,
regardless of how many categories it belongs to.

An alternative to micro-averaging is \emph{macro-averaging}, in which
the per-category scores $\pi_i$, $\rho_i$, and ${F_1}_i$ are simply
averaged to find the macro-averaged $\pi$, $\rho$, and $F_1$, notated
$\pi^M$, $\rho^M$, and $F^M_1$.  The following equations describe this
procedure.

\begin{equation*}
 \pi^M = \frac{\sum_{i=1}^{|C|}{\pi_i}}   {|C|} \qquad
\rho^M = \frac{\sum_{i=1}^{|C|}{\rho_i}}  {|C|} \qquad
 F^M_1 = \frac{\sum_{i=1}^{|C|}{{F_1}_i}} {|C|} \qquad
\end{equation*}

Macro-averaging gives equal weight to each category in the corpus,
regardless of how many documents it contains.  Thus it provides a
good counterpart to micro-averaging; macro-averaging will place more
emphasis on rare categories than micro-averaging, so reporting both
scores is typically useful to evaluate the system as a whole.

Because micro-averaging emphasizes performance on common categories,
and categorizers will typically perform better on categories with more
training examples, micro-averaged performance scores are usually
higher than macro-averaged scores.  The size of the gap between the
micro- and macro-averaged scores can be a good indicator of the
difference in performance of the system on common and rare categories.

Note that the error and accuracy measures are unaffected by micro-
vs. macro-averaging, as shown in the following derivation.  This uses
the observation that $A_i+B_i+C_i+D_i = |\test|$, a consequence of the
fact that exactly one of the terms on the left side will be
incremented with each decision about whether a document from the test
set belongs to $c_i$ or not.

\begin{equation*}
\begin{split}
error^M
 & = \frac{\sum_{i=1}^{|\cats|}{error_i}}  {|\cats|} \\[6pt]
 & = \frac{\sum_{i=1}^{|\cats|}{ \frac{B_i+C_i}{A_i+B_i+C_i+D_i} }} { \sum_{i=1}^{|\cats|}{1} } \\[6pt]
 & = \frac{\sum_{i=1}^{|\cats|}{ \frac{B_i+C_i}{|\test|} }}         { \sum_{i=1}^{|\cats|}{1} } \\[6pt]
 & = \frac{\sum_{i=1}^{|\cats|}{B_i+C_i}}                           { \sum_{i=1}^{|\cats|}{|\test|} } \\[6pt]
 & = \frac{\sum_{i=1}^{|\cats|}{B_i+C_i}}                           { \sum_{i=1}^{|\cats|}{A_i+B_i+C_i+D_i} } \\[6pt]
 & = error^\mu
\end{split}
\end{equation*}

