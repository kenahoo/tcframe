\chapter{Background: Text Categorization}
\label{background-tc}

This chapter gives an overview of the typical Text Categorization
process, including all aspects of the task that a TC framework must
consider.  It is not a design specification, but it describes both the
conceptual environment and the vocabulary necessary to understand the
design discussion in Chapter \ref{design}.

\section{The Text Categorization Process}

In order to train a categorizer as described in Section
\ref{tc-intro}, the user must begin with a \emph{training corpus},
hereafter referred to as $\train$.  This is a set of documents which
are pre-labeled with categories that are considered to be fully
correct---typically, they have been manually assigned by a
\emph{domain expert}, i.e. a person who is familiar with the type of
material contained in the documents and who knows how to assign a set
of categories to each document based on the documents' content.

The basic workflow for Text Categorization is relatively simple: the
documents in $\train$ are presented to the TC system, the system
process the documents' content, and a specific categorization
algorithm is produced that may be used to categorize future documents
from the set $\docs$.  In an application, however, many details of
this process need to be managed in specific and often varying ways.
Sections \ref{Document storage} through \ref{Hypothesis behavior}
describe the stages of this process.

\subsection{Document storage}
\label{Document storage}

In an organization that needs a TC application, documents may have
many different origins.  They may originate from plain-text or
formatted email messages, they may be raw or pre-processed web pages,
they may be collections of data from a database, or they may not have
a straightforward representation outside of the TC system at all.  It
is therefore important to recognize that the notion of varying
document storage media, and the process of converting from those media
to a medium accessible to the TC system, is an important part of the
TC process.

In addition, many Text Categorization data sets are quite large.  In
their raw format, they may commonly be larger than the amount of
available memory on the machine processing them.  This has two
important implications.  First, converting the documents to a special
storage format (for instance, as a set of files on the local
filesystem) so that the TC system can access them may be impossible or
undesirable for reasons of time, space, and/or data redundancy.
Second, a mechanism that can deal with iterating through the native
storage medium of the documents without reading all document data into
memory is probably necessary in a TC system.

\subsection{Document format}
\label{Document format}

Although most of the academic TC literature considers a document to be
a simple plain text string of data, this may rarely be the case in an
application environment.  Documents may be stored in many different
formats, including plain text, HTML, PDF, XML, Microsoft Word .doc
format, MIME-encoded email messages, and so on.  The internal data in
each document may also be considered part of its format when
nontrivial amounts of information extraction or other transformations
need to be applied to the document data in order to make it accessible
to the TC system.  For example, digit-strings in some document
collections may be useful as terms to consider when categorizing,
whereas in other collections they may only add noise to the data.

For reasons similar to those mentioned in the previous section, it may
be desirable for a TC system to deal with these issues directly, or
more likely, provide a mechanism for a developer to extend the system
in this way, rather than forcing the developer to convert all data to
a format recognized by the system.

\subsection{Document structure}

Separate from the issue of document \emph{format} is that of document
\emph{structure}.  In an age when XML data is increasingly more common
as a data exchange and storage format, the structure of a document,
i.e. the way the constituent parts of a document interrelate and nest
to create the entire document, may be important to understanding the
document's meaning.

In the TC literature, little is currently made of document structure,
except that a TC system may assign importance weights to the terms in
a document according to pre-set importance weights of the sections in
which those terms were found.  For instance, a term found in the title
of a document might be considered twice as important as a term found
in the body.  However, as research into categorization of structured
documents progresses, this may be an important area to consider in
building TC systems.

\subsection{Tokenizing of data}

The segmenting of text data into chunks representing individual words
may seem like a straightforward task, but in fact there are many
variations on how this process, called \emph{tokenizing} in this
context, can be performed.\cite[p. XXX]{manning:99} It is not
sufficient to split the data by using whitespace (spaces, tabs
returns, and the like) as delimiters, because this does not deal with
punctuation characters.  It is also not obvious whether terms with
punctuated suffixes like \emph{boy's} or \emph{doesn't}, or hyphenated terms
like \emph{ready-made} ought to be treated as one feature or multiple
features---this decision will usually need to be made with some
knowledge of the document set $\docs$.  In addition, many non-European
written languages do not contain spaces indicating divisions between
words, so a sophisticated tokenizer may be required when dealing with
languages like these.

In order to address these issues, a TC system needs to allow
variations in the tokenizing process.  This may involve providing a
set of parameters that the user can adjust in order to affect
tokenizing, or in some cases the application developer may need to
write custom code to handle domain-specific cases.

\subsection{Dimensionality reduction}

Like many Language Processing fields, much of Text Categorization has
to do with the problem of \emph{high dimensionality}.
\cite[p. 13]{sebastiani:02} \cite[p. XXX]{manning:99} The
dimensionality of the space in which the Machine Learning algorithm
must operate can be as large as the total number of distinct terms in
$\train$.  This presents two problems.  First, the Machine Learning
algorithm may scale poorly with increased dimensionality, so that
using the full feature set may be prohibitively costly.  Second, the
$\train$ data in a high-dimensionality space may be too sparse, with
not enough nonzero data points to make any useful inductive leap
during training.  This is particularly true in some highly
morphological languages like Finnish, in which a single word stem may
have thousands or millions of inflected forms, and most forms may only
be seen one time in all of $\train$, making them almost useless for
inductive learning.

One way to address the problem of high dimensionality is by applying a
linguistic \emph{stemming} algorithm to the terms found in $\train$
and $\docs$.  These algorithms transform words by removing prefix and
suffix morphemes, so that words like \emph{running} and \emph{runner}
collapse to their linguistic stem \emph{run}.  Although the use of
such processing has occasionally been reported to harm overall system
performance \cite{baker:98}, availability of such an algorithm is
usually seen as a necessary component of a TC
system.\cite[p. 12]{sebastiani:02}

Another way to reduce dimensionality in a TC system is to apply
\emph{feature selection} and/or \emph{feature extraction}.  Both are
statistical techniques to transform the entire set of document terms
into a smaller feature set with less sparsity.  The former does this
by choosing the ``most relevant'' terms using some statistical
criterion such as the number of documents each term appears in, or a
$\chi^2$ metric of term-document correlation.\cite{yang:97} The latter
applies some transformation such as singular value decomposition (used
in the ``Latent Semantic Indexing'' technique) or term clustering in
order to create a new, lower-dimensional space of features.

\subsection{Vector space modeling}

The discussion in the previous section suggests that each document may
be viewed as a vector in a global vector space whose dimensions
represent the set of all unique features from $\train$.  This idea
forms the basis for several Machine Learning techniques, including
Support Vector Machines and k-Nearest-Neighbor categorizers.  It also
allows for arbitrary vector processing algorithms on the document data
to improve categorization results.

A common set of algorithms used for this purpose is the \emph{TF/IDF}
term-weighting scheme of Salton and Buckley \cite{salton:88}, which
allows for several different ways to represent a document as a vector
in the global vector space.  Terms may be weighted by their frequency
in the document, by the logarithm of that frequency, or by a boolean
figure representing only the presence or absence of the term.  Term
weight may also be reduced by a factor representing prevalence in
other documents, and the overall length of the document vector may be
scaled in several different ways.  The TF/IDF vector weighting
techniques are used commonly in TC systems, and their availability is
desirable for the \aicat\ framework.

\subsection{Machine learning algorithm choice}



\subsection{Machine learning configuration}

\subsection{Incremental or on-line learning}

\subsection{Hypothesis behavior}
\label{Hypothesis behavior}

\section{Machine learning techniques}
\subsection{Naive Bayes}
\subsection{Support Vector Machines}
\subsection{k-Nearest-Neighbor}

\section{Related products}

To discuss the relevance of \aicat\ in the marketplace of Text
Categorization, three related products will be examined.  These
products are by no means the only available products similar to
\aicat, but they provide a reasonable sample of well-known tools for
comparison.

\subsection{Weka}

Weka is an open-source system for Machine Learning originally
developed at the University of Waikato, New Zealand, by Ian H. Witten
and Eibe Frank.\cite{weka} Its primary
audience is the international community of academic Machine Learning
researchers, most notably those working with Categorization or
Clustering problems that arise from working with text.  Weka has
undergone at least one major code rewrite; at present it is
implemented as a set of related Java classes with documented internal
interfaces, so it may itself be considered a framework.

Weka is used extensively throughout the academic Text Categorization
community, and as such includes support for many cutting-edge
categorization techniques, including advances in Support Vector
Machines, k-Nearest-Neighbor, Naive Bayes, and other categorizers, as
well as several variations of feature selection techniques.  It is
therefore a standard against which the \aicat\ framework can
be measured, as well as a resource which can be leveraged in its
construction.

Despite some similar properties, Weka and \aicat\ differ in
their goals and in many important implementation decisions.  Whereas
Weka specifically targets the academic research community,
\aicat\ aims to support use cases under both
application-building and research situations.
Consequently, Weka will typically keep up with research trends more
closely, but \aicat\ will usually be easier for application
developers to integrate into a real-world situation.

In addition to these differences, another important difference arises
from the different goals in the two projects.  Much of the academic
community is interested in evaluating the correctness and algorithmic
complexity of categorization techniques, whereas most application
developers must also consider resource usage in real-world terms like
time and memory.  In testing, \aicat\ has greatly outperformed
Weka in terms of speed and memory when equivalent algorithms are
compared on identical data sets.  This doesn't reflect an inherent
design flaw in Weka, rather a difference in the kinds of things Weka
developers are likely to spend their time working on.

In order to help facilitate cooperation between the Weka and
\aicat\ communities, as well as leverage existing solutions
inside \aicat, a machine learner class has been created
within \aicat\ that simply passes data through to Weka's
categorizers.  In this way, application developers can easily
experiment with Weka's cutting-edge categorization techniques while
retaining \aicat's application integration advantages.  Any
cross-pollination generated as a result will likely benefit both
projects.  See Section \ref{Adapter-learner} for more information on
the existing bridge to Weka.

Other facilities provided by Weka are not yet offered by
\aicat.  These include visualization tools and several
sophisticated correctness evaluation tools.  Most of these
facilities would make useful additions to \aicat\ if
implemented.

\subsection{Autonomy.com}

\subsection{Teragram Corporation}

According to their web site (\url{http://www.teragram.com/}), Teragram
Corporation is a provider of ``fast and stable linguistic
technologies, information search and extraction, knowledge management,
and text processing technologies.''  One of their largest-scale
products is the Teragram Categorizer, an automatic document
categorizer that plays a similar technical role to \aicat.
It cooperates with the Teragram Taxonomy Manager, which provides a
user interface to categories and the documents within each category.

All of Teragram's software products are proprietary, so little
information on implementation is available.  However, product
capabilities and roles can be assessed from the marketing information
given on the web site.  The information presented here has all been
gathered this way.

The Taxonomy Manager is a browser of hierarchical categories, similar
to several on-line directory services like Yahoo
(\url{http://www.yahoo.com/}) or the Open Directory Project
(\url{http://www.dmoz.org/}).  It might therefore be inferred that the
Categorizer is a native hierarchical categorizer, or perhaps that the
categorizer actually flattens the tree structure of the category
hierarchy into a flat list of its leaves, and imposes the tree
structure only afterwards.  Whichever case is true, it must be noted
that the interfaces of the categorizer allow hierarchical
categorization even if the internal workings are flat.

Another interesting aspect of Teragram's categorization technology is
their Rule-Based Categorizer.  Using this system, ``each category
within the directory is associated with a set of rules that describe
documents within that category.''  This may be motivated by a need to
integrate older hand-maintained lists of rules into newer
applications, or it might be meant to address situations like email
categorization in which most documents are indeed best categorized by
simple rules (usually because the sender and receiver have agreed upon
a tagging scheme to mark documents' important properties).  It's not
clear whether Teragram's Rule-Based Categorizer and Automatic
Categorizer can cooperate on a single taxonomy, but they indicate that
the two systems are complementary rather than antithetic.

Teragram also offers separate licensing for many of the tools that
make up its products.  In this sense, it has a strategy similar to one
employed in \aicat's design, in which useful pieces of
functionality created for \aicat\ should be split off into
their own products whenever possible.

