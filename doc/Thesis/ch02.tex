\chapter{Background: Text Categorization}
\label{background-tc}

This chapter gives an overview of the main Text-Categorization-related
considerations that an application developer must take into account
when building a TC application.  These considerations form the basis
for the design of the \aicat\ framework.  Note that this chapter does
not form a design specification, but it describes both the conceptual
environment and the vocabulary necessary to understand the design
discussion in Chapter \ref{design}.

\section{The Text Categorization Process}

In order to train a categorizer as described in Section
\ref{tc-intro}, the user must begin with a \emph{training corpus},
hereafter referred to as $\train$.  This is a set of documents which
are pre-labeled with categories that are considered to be fully
correct---typically, they have been manually assigned by a
\emph{domain expert}, i.e. a person who is familiar with the type of
material contained in the documents and who knows how to assign a set
of categories to each document based on the documents' content.

The basic workflow for Text Categorization is relatively simple: the
documents in $\train$ are presented to the TC system, the system
process the documents' content, and a specific categorization
algorithm is produced that may be used to categorize future documents
from the set $\docs$.  In an application, however, many details of
this process need to be managed in specific and often varying ways.
Sections \ref{Document storage} through \ref{Hypothesis behavior}
describe the stages of this process.

\subsection{Document storage}
\label{Document storage}

In an organization that needs a TC application, documents may have
many different origins.  They may originate from plain-text or
formatted email messages, they may be raw or pre-processed web pages,
they may be collections of data from a database, or they may not have
a straightforward representation outside of the TC system at all.  It
is therefore important to recognize that the notion of varying
document storage media, and the process of converting from those media
to a medium accessible to the TC system, is an important part of the
TC process.

In addition, many Text Categorization data sets are quite large.  In
their raw format, they may commonly be larger than the amount of
available memory on the machine processing them.  This has two
important implications.  First, converting the documents to a special
storage format (for instance, as a set of files on the local
filesystem) so that the TC system can access them may be impossible or
undesirable for reasons of time, space, and/or data redundancy.
Second, a mechanism that can deal with iterating through the native
storage medium of the documents without reading all document data into
memory is probably necessary in a TC system.

\subsection{Document format}
\label{Document format}

Although most of the academic TC literature considers a document to be
a simple plain text string of data, this may rarely be the case in an
application environment.  Documents may be stored in many different
formats, including plain text, HTML, PDF, XML, Microsoft Word .doc
format, MIME-encoded email messages, and so on.  The internal data in
each document may also be considered part of its format when
nontrivial amounts of information extraction or other transformations
need to be applied to the document data in order to make it accessible
to the TC system.  For example, digit-strings in some document
collections may be useful as terms to consider when categorizing,
whereas in other collections they may only add noise to the data.

For reasons similar to those mentioned in the previous section, it may
be desirable for a TC system to deal with these issues directly, or
more likely, provide a mechanism for a developer to extend the system
in this way, rather than forcing the developer to convert all data to
a format recognized by the system.

\subsection{Document structure}

Separate from the issue of document \emph{format} is that of document
\emph{structure}.  In an age when XML data is increasingly more common
as a data exchange and storage format, the structure of a document,
i.e. the way the constituent parts of a document interrelate and nest
to create the entire document, may be important to understanding the
document's meaning.

In the TC literature, little is currently made of document structure,
except that a TC system may assign importance weights to the terms in
a document according to pre-set importance weights of the sections in
which those terms were found.  For instance, a term found in the title
of a document might be considered twice as important as a term found
in the body.  However, as research into categorization of structured
documents progresses, this may be an important area to consider in
building TC systems.

\subsection{Tokenizing of data}

The segmenting of text data into chunks representing individual words
may seem like a straightforward task, but in fact there are many
variations on how this process, called \emph{tokenizing} in this
context, can be performed.\cite[p. XXX]{manning:99} It is not
sufficient to split the data by using whitespace (spaces, tabs
returns, and the like) as delimiters, because this does not deal with
punctuation characters.  It is also not obvious whether terms with
punctuated suffixes like \emph{boy's} or \emph{doesn't}, or hyphenated terms
like \emph{ready-made} ought to be treated as one feature or multiple
features---this decision will usually need to be made with some
knowledge of the document set $\docs$.  In addition, many non-European
written languages do not contain spaces indicating divisions between
words, so a sophisticated tokenizer may be required when dealing with
languages like these.

In addition, the tokenization process (or equivalently, a stage
directly after tokenization) may remove from the data any term in a
pre-defined list of ``stop words,'' which are words that occur very
commonly in the domain (such as ``the'' or ``and'' in most English
texts) and are assumed to contain little or no relevance to the
categorization problem at hand. \ref[p. 15]{sebastiani:02}
\ref{mladenic:98}

In order to address these issues, a TC system needs to allow
variations in the tokenizing process.  This may involve providing a
set of parameters that the user can adjust in order to affect
tokenizing, or in some cases the application developer may need to
write custom code to handle domain-specific cases.

\subsection{Dimensionality reduction}

Like many Language Processing fields, much of Text Categorization has
to do with the problem of \emph{high dimensionality}.
\cite[p. 13]{sebastiani:02} \cite[p. XXX]{manning:99} \cite{joachims:98} The
dimensionality of the space in which the Machine Learning algorithm
must operate can be as large as the total number of distinct terms in
$\train$.  This presents two problems.  First, the Machine Learning
algorithm may scale poorly with increased dimensionality, so that
using the full feature set may be prohibitively costly.  Second, the
$\train$ data in a high-dimensionality space may be too sparse, with
not enough nonzero data points to make any useful inductive leap
during training.  This is particularly true in some highly
morphological languages like Finnish, in which a single word stem may
have thousands or millions of inflected forms, and most forms may only
be seen one time in all of $\train$, making them almost useless for
inductive learning.

One way to address the problem of high dimensionality is by applying a
linguistic \emph{stemming} algorithm to the terms found in $\train$
and $\docs$.  These algorithms transform words by removing prefix and
suffix morphemes, so that words like \emph{running} and \emph{runner}
collapse to their linguistic stem \emph{run}.  Although the use of
such processing has occasionally been reported to harm overall system
performance \cite{baker:98}, availability of such an algorithm is
usually seen as a necessary component of a TC
system.\cite[p. 12]{sebastiani:02}

Another way to reduce dimensionality in a TC system is to apply
\emph{feature selection} and/or \emph{feature extraction}.  Both are
statistical techniques to transform the entire set of document terms
into a smaller feature set with less sparsity.  The former does this
by choosing the ``most relevant'' terms using some statistical
criterion to measure relevance.  The latter
applies some transformation such as singular value decomposition (used
in the ``Latent Semantic Indexing'' technique) or grouping terms into clusters in
order to create a new, lower-dimensional space of features.

Three of the most commonly used feature selection criteria are the
\emph{information gain} (IG), \emph{document frequency} (DF), and
\emph{$\chi^2$} metrics.\cite{yang:97} $\text{DF}(f_k)$ is simply the
number of documents of $\train$ in which the feature $f_k$ occurs.
$\chi^2$ is defined as follows---here, $A$, $B$, $C$, and $D$ are
defined as the terms in the contingency table shown in Table
\ref{termcat-contingency}.

\begin{align*}
\chi^2(f_k,c_j) & = \frac{|\train|(AD-CB)^2}{(A+C)(B+D)(A+B)(C+D)} \\
\end{align*}


\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\cline{3-4}
\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{$c_j$ \textbf{occurs}} \\
\cline{3-4}
\multicolumn{2}{c|}{} & Yes & No \\
\hline
$f_k$           & Yes & $A$ & $B$ \\
\cline{2-4}
\textbf{occurs} & No  & $C$ & $D$ \\
\hline
\end{tabular}
\end{center}
\caption{Contingency table for category $c_j$ and term $f_k$.  The
  quantities $A$-$D$ represent the number of documents with the given
  properties.}
\label{termcat-contingency}
\end{table}

In order to find the overall $\chi^2(f_k)$ metric, the terms
$\chi^2(f_k,c_j)$ may either be averaged (typically weighted by the
frequency of each category), or the maximum value for any category may
be adopted.\cite{yang:97}

$IG$ is defined as follows, where $P(f_k)$ and $P(\overline{f_k})$ are
the probabilities that a document does or does not contain $f_k$,
respectively, $\cats_{f_k}$ and $\cats_{\overline{f_k}}$ are the
category sets in the subsets of $\docs$ containing $f_k$ or not,
respectively, and $\Entr(x)$ is the standard entropy function from
Information Theory. \cite[ch. 2]{manning:99}

% XXX - need to check notation for conditional entropy in Manning
\begin{equation*}
        IG(f_k) = \Entr(\cats)
                  - P(f_k) \Entr(\cats_{f_k})
	          - P(\overline{f_k}) \Entr(\cats_{\overline{f_k}})
%        IG(f_k) = - \Entr(\cats) + P(f_k) \Entr(\cats|f_k)
%	          + P(\overline{f_k}) \Entr(\cats|\overline{f_k})
\end{equation*}


\subsection{Vector space modeling}

The discussion in the previous section suggests that each document may
be viewed as a vector in a global vector space whose dimensions
represent the set of all unique features from $\train$.  This idea
forms the basis for several Machine Learning techniques, including
Support Vector Machines and k-Nearest-Neighbor categorizers.  It also
allows for arbitrary vector processing algorithms on the document data
to improve categorization results.

A common set of algorithms used for this purpose is the \emph{TF/IDF}
term-weighting scheme of Salton and Buckley \cite{salton:88}, which
allows for several different ways to represent a document as a vector
in the global vector space.  Terms may be weighted by their frequency
in the document, by the logarithm of that frequency, or by a boolean
figure representing only the presence or absence of the term.  Term
weight may also be reduced by a factor representing prevalence in
other documents, and the overall length of the document vector may be
scaled in several different ways.  The TF/IDF vector weighting
techniques are used commonly in TC systems, and their availability is
desirable for the \aicat\ framework.

\subsection{Machine Learning algorithm}

Many different Machine Learning algorithms are actively studied in the
TC research literature, and new algorithms or variations on existing
algorithms are continually being developed.  In addition, the choice
of algorithm may depend on the specific application---algorithms
differ not only in their ability to perform accurately on differing
data sets, but also in the resources they may require during training
and when categorizing documents.  Therefore, it is not possible to
choose a single Machine Learning algorithm for incorporation into the
\aicat\ framework.  As a TC system, it needs to allow for selection
among several standard algorithms and for incorporation of novel
algorithms developed by researchers.

Section \ref{machine-learning} gives an overview of three well-studied
Machine Learning algorithms and compares some of their relevant
characteristics.  Section \ref{Strategy} in Chapter \ref{design} shows
how the architecture of \aicat\ allows for flexibility in this aspect
of categorization.

\subsection{Machine Learning configuration}
\label{ml-config}

Even within a single Machine Learning algorithm, there may be several
parameters that a supervisor may vary to influence the training and
categorization processes.  For instance, the k-Nearest-Neighbor
algorithm has an adjustable parameter $k$, the SVM algorithm allows
for several variations in the type of kernel used, and most
categorization algorithms admit some type of control to trade off
precision and recall against each other (see Section \ref{measures}
for an explanation of these terms).  In order to achieve the
appropriate performance for a given task, application developers need
simple ways to vary these parameters.

In fact, this issue is not unique to the Machine Learning component of
the TC process.  Several of the previously discussed aspects of the TC
task, including feature selection, dimensionality reduction, and
vector space transformation, may be controlled by parameters that the
supervisor may wish to vary.  Consistency in the system's handling of
parameters may therefore be an important part of its design.  This
issue will be discussed again in Section \ref{factory-method}.

\subsection{Incremental learning}

In some TC applications, it may be desirable to incorporate feedback
from the user about whether the system's categorization decisions have
been correct or incorrect.\cite[p. 28]{sebastiani:02} This may allow
for a relatively small initial training set $\train$, or for
categorization on concepts which may change over time.  This process
is called \emph{incremental} or \emph{on-line} learning.

Unfortunately, incremental learning is not possible with all Machine
Learning methods, since some algorithms (e.g. Neural Network
categorizers) may not be able to incorporate new evidence into their
model without going through the entire training process again.  For
those algorithms which can support it, however, the use of incremental
learning may be considered important in building a TC application, and
is therefore considered a goal of the \aicat\ project.

\subsection{Hypothesis behavior}
\label{Hypothesis behavior}

Most of the standard Text Categorization literature assumes that the
goal of TC is to assign each document to one of two mutually exclusive
categories, otherwise known as \emph{binary
categorization}.\cite[p. 3]{sebastiani:02} Of course, real-world
problems often involve ontologies that consist of more than two
categories, and membership may not be mutually exclusive.  For
instance, some applications may require assigning only the most
appropriate category from $\cats$, some may require assigning any
appropriate category, and some, such as rank-based tasks, may require
an appropriateness score linking each category-document pair.

This situation does not represent a theoretical disconnect between
research and practice, because each multi-category TC problem can be
re-posed as a series of binary problems.  Most application builders,
however, will not want to actually re-pose their problems in this
manner, because it requires extra work, and it may obscure the true
nature of the application under development.  Therefore, it is
desirable for a TC system to offer support for all the scenarios
described in the previous paragraph, without requiring the
implementers of the Machine Learning algorithms to explicitly code for
them.  The way in which this is achieved is discussed in Section
\ref{class-overview}.

\section{Machine Learning techniques}
\label{machine-learning}

This section describes three Machine Learning techniques that are
common for Text Categorization: \naive\ Bayes categorizers, Support
Vector Machines, and k-Nearest-Neighbor categorizers.

\subsection{\naive\ Bayes}

\naive Bayes categorizers are extremely well-represented in the TC
literature \cite{lewis:98,yang:99,sebastiani:02}, often giving a
baseline performers against which other categorizers can be measured.
Their theory rests on Bayes' Theorem of conditional probability, shown
in Equation \ref{bayes}.  For those unfamiliar with conditional
probabilities, the notation $P(a|b)$ means ``the probability of $a$
given $b$.''

\begin{equation} \label{bayes}
P(x|y) = \frac{P(y|x) P(x)}{P(y)}
\end{equation}

The quantity of interest when determining the relevance of a
particular document $d_i$ to a category $c_j$ is $P(c_j|d_i)$.  Any
category with a high enough conditional probability will be considered
assigned to $d_i$.  In particular, the ``best'' category will be
$\ArgMax_{c_j\in\cats} P(c_j|d_i)$. This probability is usually impossible to
compute directly, however, because $d_i$ has likely never been
encountered before.  Therefore, Bayes' Theorem can be applied to
change the probabilistic expression to one whose terms may be
estimated from the training data $\train$ as follows.

\begin{align*}
\ArgMax_{c_j\in\cats} P(c_j|d_i) 
 & = \ArgMax_{c_j\in\cats} \frac{P(d_i|c_j) P(c_j)}{P(d_i)} & (\text{by (\ref{bayes})}) \\
 & = \ArgMax_{c_j\in\cats} P(d_i|c_j) P(c_j) & (P(d_i) \text{ is constant}) \\
\end{align*}

$P(c_j)$ may be easily estimated from the frequency with which
documents appear in $c_j$ in $\train$.  To estimate $P(d_i|c_j)$,
$d_i$ may be considered equivalent to the string of its features
$f_{i1} f_{i2} \ldots f_{ik}$.  $\ArgMax_{c_j\in\cats} P(c_j|d_i)$ may
then be estimated as follows.

\begin{align*}
\ArgMax_{c_j\in\cats} P(c_j|d_i) 
 & = \ArgMax_{c_j\in\cats} P(f_{i1} \ldots f_{ik}|c_j) P(c_j) \\
 & \approx \ArgMax_{c_j\in\cats} P(f_{i1}|c_j) \cdot \ldots \cdot P(f_{ik}|c_j) P(c_j) \\
\end{align*}

This final step, which gives this algorithm its ``na\"ive'' moniker,
involves two conditional independence assumptions: first, that the
features $f_{i1}, \ldots, f_{ik}$ are conditionally independent given
the category $c_j$, and second, that the position of features within
document $d_i$ has no effect on the probability.  These features may
not be true in general---features may in fact correlate in complex
ways in real-world documents.  Nevertheless, the \naive\ Bayes
categorizer tends to produce fairly good results, and an analysis of
this phenomenon can be found in \cite{domingos:97}.

The conditional probabilities $P(f_{i1}|c_j), \ldots, P(f_{ik}|c_j)$
are typically estimated by measuring the relative frequencies of the
features $f_{i1}, \ldots, f_{ik}$ in the documents belonging to
$c_j$.  For features found in the documents of $\docs$ which were not
encountered in $\train$, it would be inappropriate to use this
estimate, however, because it would yield a probability of zero and
render the rest of the terms useless.  For this reason, unknown terms
are typically assigned some small nonzero probability in a process
known as \emph{smoothing}.

The \naive\ Bayes algorithm is fairly fast and non-memory-intensive.
Because its training process consists merely of counting the features
of the training documents, its training time scales linearly with
$|\train|$.  Categorization is also fairly fast, because all the
pre-computed probabilities $P(f_{il}|c_j)$ may simply be looked up in
an array.  Categorization of a single document therefore scales
linearly with $|\cats|$.  Because the system need only store feature
information on a per-category basis instead of a per-document basis,
the size of the trained categorizer will stay fairly small compared to
more resource-intensive categorizers like k-Nearest-Neighbor.

\subsection{k-Nearest-Neighbor}
\label{knn}

The k-Nearest-Neighbor algorithm (kNN) is one of the most conceptually
simple TC algorithms in the literature.  All documents in $\train$ are
considered as vectors in a space with a similarity measure $m: \docs
\times \docs \to \mathbb{R}$.  To determine whether an unseen document
$d_i$ is assigned to a category $c_j$, the $k$ most similar documents
to $d_i$ using the measure $m$ are determined, where $k$ is a
user-adjustable parameter.  If the number of these $k$ documents that
belong to $c_j$ (possibly weighted by the similarity measure $m$ for
each similar document) is greater than some pre-defined threshold,
then $d_i$ is assigned to $c_j$, and otherwise not.  This technique
has been described in \cite[p. 28]{sebastiani:02}, \cite{yang:99}, and
\cite{yang:97}, among others.

The choices for the $k$ parameter, the category-membership threshold,
the similarity function $m$, and how to map from the similarity scores
$m$ to the overall score for $c_j$ provide for many variations on the
standard algorithm.  For instance, a single membership threshold may
be used for all categories, or a different threshold may be used for
each category, possibly learned by optimizing performance on a
validation set.  In addition, if more than one document is being
categorized in a batch operation, several differing strategies for
thresholding may be employed that take advantage of the overall
proportions of documents that belong to each category.\cite{yang:01}

Although the k-Nearest-Neighbor algorithm is conceptually simple, it
is computationally intensive.  Unless thresholds are learned from a
validation set, there is no actual training stage when building a
categorizer---all decisions are made by computations performed during
categorization.  The time to train a kNN categorizer is therefore
minimal or null, but the time to categorize a single unseen document
scales linearly with $|\train|$ and must be performed in full for each
categorization.  In addition, the entire training corpus $\train$ must
be preserved in the categorizer's model, so memory or storage
requirements may be prohibitively high in some environments.

\subsection{Support Vector Machines}

Support Vector Machines (SVM) are another extremely active area of
research in the Text Categorization literature.  Their use in TC was
introduced by \cite{joachims:98}, and several studies, including
\cite{joachims:98} and \cite{yang:99}, have found their results to be
highly competitive with other Machine Learning methods on the standard
benchmark corpora.

SVM techniques are similar to kNN in that they view the training
documents as vectors in a vector space, and that they require a
similarity function (called the ``kernel'' function) that plays a role
similar to the function $m$ in Section
\ref{knn}.\cite[ch. 1]{scholkopf:02} However, instead of considering
the documents most similar to the document to be categorized, SVM
algorithms learn a \emph{decision surface} during training which
divides the vector space into regions that indicate category
membership.  Categorization then simply consists of determining which
side of the decision surface each document to be categorized lies on.

One key advantage of SVMs is that they can deal well with very large
feature spaces, both in terms of the correctness of the categorization
results and the efficiency of the training and categorization
algorithms.  This implies that little or no feature selection may need
to be performed on the training data, removing a possibly
time-consuming aspect of the TC process.  Unfortunately, a
disadvantage of many SVM training algorithms is that they scale poorly
with $|\train|$, in some cases requiring as much as $O(|\train|^3)$ or
$O(|\train|^4)$.  This may make their use with large numbers of
documents prohibitive unless the standard algorithms are modified.


\section{Related products}

To discuss the relevance of \aicat\ in the marketplace of Text
Categorization, three related products are examined here.  These
products are by no means the only available products similar to
\aicat, but they provide a reasonable sample of well-known tools for
comparison.

\subsection{Weka}

Weka is an open-source system for Machine Learning originally
developed at the University of Waikato, New Zealand, by Ian H. Witten
and Eibe Frank.\cite{weka:99} Its primary
audience is the international community of academic Machine Learning
researchers, most notably those working with Categorization or
Clustering problems that arise from working with text.  Weka has
undergone at least one major code rewrite; at present it is
implemented as a set of related Java classes with documented internal
interfaces, so it may itself be considered a framework.

Weka is used extensively throughout the academic Text Categorization
community, and as such includes support for many cutting-edge
categorization techniques, including advances in Support Vector
Machines, k-Nearest-Neighbor, \naive\ Bayes, and other categorizers, as
well as several variations of feature selection techniques.  It is
therefore a standard against which the \aicat\ framework can
be measured, as well as a resource which can be leveraged in its
construction.

Despite some similar properties, Weka and \aicat\ differ in
their goals and in many important implementation decisions.  Whereas
Weka specifically targets the academic research community,
\aicat\ aims to support use cases under both
application-building and research situations.
Consequently, Weka will typically keep up with research trends more
closely, but \aicat\ will usually be easier for application
developers to integrate into a real-world situation.

In addition to these differences, another important difference arises
from the different goals in the two projects.  Much of the academic
community is interested in evaluating the correctness and algorithmic
complexity of categorization techniques, whereas most application
developers must also consider resource usage in real-world terms like
time and memory.  In testing, \aicat\ has greatly outperformed
Weka in terms of speed and memory when equivalent algorithms are
compared on identical data sets.  This doesn't reflect an inherent
design flaw in Weka, rather a difference in the kinds of things Weka
developers are likely to spend their time working on.

In order to help facilitate cooperation between the Weka and
\aicat\ communities, as well as leverage existing solutions
inside \aicat, a machine learner class has been created
within \aicat\ that simply passes data through to Weka's
categorizers.  In this way, application developers can easily
experiment with Weka's cutting-edge categorization techniques while
retaining \aicat's application integration advantages.  Any
cross-pollination generated as a result will likely benefit both
projects.  See Section \ref{Adapter-learner} for more information on
the existing bridge to Weka.

Other facilities provided by Weka are not yet offered by
\aicat.  These include visualization tools and several
sophisticated correctness evaluation tools.  Most of these
facilities would make useful additions to \aicat\ if
implemented.

\subsection{Autonomy.com}

Autonomy Corporation (\url{http://www.autonomy.com/}) provides
information services and product licensing to enterprise-level
organizations.  Some of its customers include General Motors,
Ericsson, Sybase, Deutsche Bank, and the United States Department of
Homeland Security.  Its products range broadly over several areas of
Text Processing and Information Retrieval, including categorization,
summarization, and search systems.  The company's web site claims that
their products are ``automatic, language independent, fast, scalable,
and accurate.''  Since the products are proprietary, no independent
verification of these claims has been done in this study, but the
claims do provide a list of attributes this company feels are
important in marketing its products.

The Autonomy web site indicates that its products utilize ``Bayesian
Inference and Claude Shannon's principles of information theory.''
While further details are not provided, this statement would be
consistent with \naive\ Bayes categorization and an Information Gain
feature selection criterion.  However, one must be cautious in making
assessments like this, since there are other ways of employing
Bayesian techniques for categorization that have not been as widely
reported in the research literature as \naive\ Bayes, and Shannon's
information theory pervades many areas of TC and Information
Retrieval, including Decision Tree construction
\cite{quinlan:89,wallace:93} and search relevance ranking
\cite{greiff:00}.

Autonomy suggests that their products can be useful in building
customized portals, Customer Relationship Management (CRM) systems,
enterprise-level search systems and document management tools, and
Human Resources solutions.  These are commonly encountered
applications mentioned (but seldom illustrated) in the TC literature,
and it seems to be generally felt that TC technologies apply broadly
to these application areas.

\subsection{Teragram Corporation}

According to their web site (\url{http://www.teragram.com/}), Teragram
Corporation is a provider of ``fast and stable linguistic
technologies, information search and extraction, knowledge management,
and text processing technologies.''  One of their largest-scale
products is the Teragram Categorizer, an automatic document
categorizer that plays a similar technical role to \aicat.
It cooperates with the Teragram Taxonomy Manager, which provides a
user interface to categories and the documents within each category.

All of Teragram's software products are proprietary, so little
information on implementation is available.  However, product
capabilities and roles can be assessed from the marketing information
given on the web site.  The information presented here has all been
gathered this way.

The Taxonomy Manager is a browser of hierarchical categories, similar
to several on-line directory services like Yahoo
(\url{http://www.yahoo.com/}) or the Open Directory Project
(\url{http://www.dmoz.org/}).  It might therefore be inferred that the
Categorizer is a native hierarchical categorizer, or perhaps that the
categorizer actually flattens the tree structure of the category
hierarchy into a flat list of its leaves, and imposes the tree
structure only afterwards.  Whichever case is true, it must be noted
that the interfaces of the categorizer allow hierarchical
categorization even if the internal workings are flat.

Another interesting aspect of Teragram's categorization technology is
their Rule-Based Categorizer.  Using this system, ``each category
within the directory is associated with a set of rules that describe
documents within that category.''  This may be motivated by a need to
integrate older hand-maintained lists of rules into newer
applications, or it might be meant to address situations like email
categorization in which most documents are indeed best categorized by
simple rules (usually because the sender and receiver have agreed upon
a tagging scheme to mark documents' important properties).  It's not
clear whether Teragram's Rule-Based Categorizer and Automatic
Categorizer can cooperate on a single taxonomy, but they indicate that
the two systems are complementary rather than antithetic.

Teragram also offers separate licensing for many of the tools that
make up its products.  In this sense, it has a strategy similar to one
employed in \aicat's design, in which useful pieces of
functionality created for \aicat\ should be split off into
their own products whenever possible.

