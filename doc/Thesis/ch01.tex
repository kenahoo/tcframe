\chapter{Introduction}

\section{Automatic Text Categorization}
\label{tc-intro}

The field of automatic Text Categorization (TC) is an extremely active
area of current research and application.  It is multi-disciplinary,
attracting attention from the Linguistics, Computer Science,
Engineering, and Business communities.  Its applicability is broad,
with many potential uses for large businesses as well as individuals.
A recent survey article from the Association of Computing Machinery
provides a good introduction to the field. \cite{sebastiani:02}

The goal of automatic Text Categorization is to create systems that
can automatically place text-based documents into predefined
categories.  For example, one system may assign general-interest news
stories themes such as ``sports,'' ``finance,'' or ``politics.''
Another system may automatically categorize an email user's email
messages by routing them into folders.  In these scenarios, the news
story or email message plays the role of a ``document,'' and the news
theme or email folder plays the role of a ``category.''  The system's
categorization decisions may be based on any arbitrary properties of
the documents, usually (though not always) some analysis of the text
content of each document.

The standard modern approach to TC involves using Machine Learning to
create categorizers automatically, rather than manually specifying the
criteria that a document must fulfill in order to belong to each
category. \cite[p. 2]{sebastiani:02} The Machine Learning process that
creates categories typically examines a set of documents which have
been pre-assigned with categories, and makes inductive abstractions
based on this data that will assist it in categorizing future
documents. \cite[ch. XXX]{mitchell:97}

Because the process of creating categorizers is automatic, and the
categorization process itself is also automatic, efficient TC systems
requiring no human intervention can be created that process large
numbers of documents very quickly.  In practice, human intervention
may sometimes be applied in either phase, because manual tuning of the
parameters that govern the creating of a categorizer may improve its
performance, and because a human expert may assist the categorizer in
making decisions, or vice versa.

The following quotation from \cite{sebastiani:02} provides a sense of
the broad range of applications currently using TC methods:

\begin{quote}
TC is now being applied in many contexts, ranging from document
indexing based on a controlled vocabulary, to document filtering,
automated metadata generation, word sense disambiguation, population
of hierarchical catalogues of Web resources, and in general any
application requiring document organization or selective and adaptive
document dispatching.
\end{quote}

Because of the recent explosion in volume of electronic data due to
the advent of the World Wide Web and the widespread use of email for
business and personal communication, many new applications may benefit
from using TC methods.  Two application areas investigated during the
course of this candidature include the use of TC methods to determine
potential market impact of corporate financial announcements
\cite{calvo:02}, and to assist educational mentors in managing a
stream of messages sent to a mathematics question-and-answer service
\cite{williams:02}.  These tasks are currently performed by humans
with special knowledge about the particular relationships between
documents and categories, and any gains in efficiency brought by
automation may significantly aid the business processes of such
organizations.

\section{Object-Oriented Application Frameworks}

An \term{Object-Oriented Application Framework} (hereafter referred to
simply as a \term{framework}) is a large-scale unit of reusable code
in object-oriented software development.\footnote{This thesis assumes
  that the reader is familiar with the basic terminology of
  object-oriented programming.  For an introduction to the subject,
  please see \cite{wirfs:90} for an academic treatment, or
  \cite{conway:99} for an applied treatment.}  The relevant software
engineering literature contains several different definitions of the
term, with two definitions appearing most commonly:

\begin{itemize}
\item a reusable design of all or part of a system that is represented
  by a set of abstract classes and the way their instances interact
  \cite{johnson:97}
\item a reusable, ``semi-complete'' application that can be
  specialized to produce custom applications \cite{fayad:97}
\end{itemize}

These definitions are not in conflict, but rather emphasize different
aspects of framework development---the first definition emphasizes
what a framework is made of, while the second emphasizes what a
framework is used for.  Notice that the first definition refers to the
system's \emph{design}, while the second refers to the system's actual
\emph{code}.  This is because frameworks represent both code reuse and
design reuse.  The design of a system gets reused because any
application built using the framework will embody the design decisions
encoded in the framework structure, and the system code gets reused by
employing the concrete classes provided with the framework.
\cite[ch. 1]{gamma:95}

In designing frameworks, developers strive to create a product that is
useful in a maximum number of situations with a minimum of effort by
the application developers.  This is evidenced by the consistent
appearance of the word ``reusable'' in the definitions in the previous
section.  In order to achieve effective reuse,
the framework developer must identify those aspects of the target
applications that vary from one application to the next, typically
called \term{hot spots}, and allow explicitly for their variations to
be instantiated in applications. \cite[ch. 14]{fayad:99}
\cite{demeyer:97}

In some cases, the hot spot
variations are known in advance to the framework developer, so
concrete classes may be provided to the application developer to
fulfill the variation requirements.  Application development then
becomes a simple matter of selecting the appropriate concrete classes
for the application.  This is known as \term{blackbox} framework
usage.

In other cases, the application developer may have a particular need
that the framework developer did not or could not anticipate.
Application development then involves writing custom subclasses of the
hot spot classes, a process known as \term{whitebox} framework
usage. \cite[ch. 1]{fayad:99}

Because blackbox framework usage involves much less effort than
whitebox usage, most framework developers aim to provide blackbox
functionality whenever possible.  Since framework requirements may not
be clear until the framework has been used in several different
applications, however, many frameworks evolve from being primarily
whitebox frameworks to primarily blackbox frameworks as they
mature. \cite[ch. 6]{gamma:95}

Frameworks are certainly not the only kind of software reuse technique
in active use.  Other reuse techniques include \term{components},
\term{libraries}, and \term{application generators}.  A component is
an element of a software system that can be replaced by other elements
with similar purpose but different behavior. \cite{johnson:97}  A
library is a set of routines or objects (possibly components) that
provide functionality developers may use in application
code. \cite[ch. 1]{fayad:99}  An application generator is a system
that creates varying applications based on high-level, domain-specific
languages that specify the desired behavior of the application in its
aspects that vary (i.e., in its hot spots). \cite[ch. 1]{fayad:99}

The biggest difference between frameworks and the above reuse
techniques is that frameworks create an \term{inversion of control}
between the framework code and the application developer code.
Framework code assumes control of the main flow in an application, and
any custom developer code (if the framework is being used in a
whitebox development style) is invoked by the framework.  In the other
reuse techniques mentioned above, the developer writes the high-level
application code (whether in a programming language or in the
application generator's mini-language) and invokes the reusable
elements at a lower level.  The inversion of control in frameworks
lets the framework developer dictate the overall structure of the
application, while allowing the application developer low-level
control over application details. \cite[ch. 1]{fayad:99}

According to \cite[ch. 1]{fayad:99}, framework methodology offers the
following benefits as a reuse technique:

\begin{description}
\item[Modularity] The hot spots of a framework represent encapsulated
  solutions to the variations in the application domain.  This helps
  minimize the impact of design and implementation changes in
  applications, because they will usually be limited to these
  encapsulated areas.
\item[Reusability] Frameworks represent both design reuse and code
  reuse, leveraging both the expertise of the framework developer
  encoded in the framework architecture, and well-tested
  implementations encoded in the framework's concrete classes.  In the
  case of blackbox usage, reuse may be achieved with no custom
  application code.
\item[Extensibility] Whitebox reuse allows frameworks to be used for
  purposes that the framework developer did not or could not foresee,
  and allows application developers to create interfaces to
  proprietary or non-generic entities while using the framework
  general architecture.
\item[Inversion of control] Custom application code can play a
  subordinate role to generic framework code, so that different
  applications developed using the same framework will behave in
  similar ways at the highest levels.
\end{description}


\subsection{Design patterns}
\label{patterns}

In order to shed light on the design of complex object-oriented
systems, many researchers and software developers have tried to
standardize language, concepts, and notation for class and object
relationships.  There is as yet no universally accepted terminology
for describing these relationships, but one common practice is to use
so-called ``design patterns'' to provide a baseline grammar for
discussing commonly seen patterns of cooperation in object-oriented
design. \cite[p. 3]{gamma:95} The design patterns do not provide
prescriptions for software design, but rather descriptions of common
practices in common situations.  Most design patterns in
\cite{gamma:95} include discussions of various trade-offs in their
application, indicating that a design pattern is actually a family of
similar solutions to a problem, not one rigid solution.

Design patterns help to illustrate object-oriented software designs
that use \emph{composition} rather than just \emph{inheritance} for
embodying important relationships between objects.  Composition refers
to the practice of multiple independent objects cooperating to achieve
a task, or assembling to form a larger functional unit, while
inheritance refers to the practice of defining a single object's
structure and behavior in terms of both general (``parent'') and
specific (``child'') specifications.  In the language of framework
design and reuse, composition allows for blackbox reuse, while pure
inheritance forces whitebox reuse.\cite[p. 19]{gamma:95}

The relevance of several design patterns to \aicat\ will be discussed
in detail in Chapter \ref{design}.

\section{Related products}

XXX - need to scan this section for forward references and insert
linkers to them like ``(see Section 3.4.5)''

To discuss the relevance of \aicat\ in the marketplace of Text
Categorization, three related products are examined here.  These
products are by no means the only available products similar to
\aicat, but they provide a reasonable sample of well-known tools for
comparison.

\subsection{Weka}

Weka is an open-source system for Machine Learning originally
developed at the University of Waikato, New Zealand, by Ian H. Witten
and Eibe Frank.\cite{weka:99} Its primary
audience is the international community of academic Machine Learning
researchers, most notably those working with Categorization or
Clustering problems that arise from working with text.  Weka has
undergone at least one major code rewrite; at present it is
implemented as a set of related Java classes with documented internal
interfaces, so it may itself be considered a framework.

Weka is used extensively throughout the academic Text Categorization
community, and as such includes support for many cutting-edge
categorization techniques, including advances in Support Vector
Machines, k-Nearest-Neighbor, \naive\ Bayes, and other categorizers, as
well as several variations of feature selection techniques.  It is
therefore a standard against which the \aicat\ framework can
be measured, as well as a resource which can be leveraged in its
construction.

Despite some similar properties, Weka and \aicat\ differ in
their goals and in many important implementation decisions.  Whereas
Weka specifically targets the academic research community,
\aicat\ aims to support use cases under both
application-building and research situations.
Consequently, Weka will typically keep up with research trends more
closely, but \aicat\ will usually be easier for application
developers to integrate into a real-world situation.

In addition to these differences, another important difference arises
from the different goals in the two projects.  Much of the academic
community is interested in evaluating the correctness and algorithmic
complexity of categorization techniques, whereas most application
developers must also consider resource usage in real-world terms like
time and memory.  In testing, \aicat\ has greatly outperformed
Weka in terms of speed and memory when equivalent algorithms are
compared on identical data sets.  This doesn't reflect an inherent
design flaw in Weka, rather a difference in the kinds of things Weka
developers are likely to spend their time working on.

In order to help facilitate cooperation between the Weka and
\aicat\ communities, as well as leverage existing solutions
inside \aicat, a machine learner class has been created
within \aicat\ that simply passes data through to Weka's
categorizers.  In this way, application developers can easily
experiment with Weka's cutting-edge categorization techniques while
retaining \aicat's application integration advantages.  Any
cross-pollination generated as a result will likely benefit both
projects.  See Section \ref{Adapter-learner} for more information on
the existing bridge to Weka.

Other facilities provided by Weka are not yet offered by
\aicat.  These include visualization tools and several
sophisticated correctness evaluation tools.  Most of these
facilities would make useful additions to \aicat\ if
implemented.

\subsection{Autonomy.com}

Autonomy Corporation (\url{http://www.autonomy.com/}) provides
information services and product licensing to enterprise-level
organizations.  Some of its customers include General Motors,
Ericsson, Sybase, Deutsche Bank, and the United States Department of
Homeland Security.  Its products range broadly over several areas of
Text Processing and Information Retrieval, including categorization,
summarization, and search systems.  The company's web site claims that
their products are ``automatic, language independent, fast, scalable,
and accurate.''  Since the products are proprietary, no independent
verification of these claims has been done in this study, but the
claims do provide a list of attributes this company feels are
important in marketing its products.

The Autonomy web site indicates that its products utilize ``Bayesian
Inference and Claude Shannon's principles of information theory.''
While further details are not provided, this statement would be
consistent with \naive\ Bayes categorization and an Information Gain
feature selection criterion.  However, one must be cautious in making
assessments like this, since there are other ways of employing
Bayesian techniques for categorization that have not been as widely
reported in the research literature as \naive\ Bayes, and Shannon's
information theory pervades many areas of TC and Information
Retrieval, including Decision Tree construction
\cite{quinlan:89,wallace:93} and search relevance ranking
\cite{greiff:00}.

Autonomy suggests that their products can be useful in building
customized portals, Customer Relationship Management (CRM) systems,
enterprise-level search systems and document management tools, and
Human Resources solutions.  These are commonly encountered
applications mentioned (but seldom illustrated) in the TC literature,
and it seems to be generally felt that TC technologies apply broadly
to these application areas.

\subsection{Teragram Corporation}

According to their web site (\url{http://www.teragram.com/}), Teragram
Corporation is a provider of ``fast and stable linguistic
technologies, information search and extraction, knowledge management,
and text processing technologies.''  One of their largest-scale
products is the Teragram Categorizer, an automatic document
categorizer that plays a similar technical role to \aicat.
It cooperates with the Teragram Taxonomy Manager, which provides a
user interface to categories and the documents within each category.

All of Teragram's software products are proprietary, so little
information on implementation is available.  However, product
capabilities and roles can be assessed from the marketing information
given on the web site.  The information presented here has all been
gathered this way.

The Taxonomy Manager is a browser of hierarchical categories, similar
to several on-line directory services like Yahoo
(\url{http://www.yahoo.com/}) or the Open Directory Project
(\url{http://www.dmoz.org/}).  It might therefore be inferred that the
Categorizer is a native hierarchical categorizer, or perhaps that the
categorizer actually flattens the tree structure of the category
hierarchy into a flat list of its leaves, and imposes the tree
structure only afterwards.  Whichever case is true, it must be noted
that the interfaces of the categorizer allow hierarchical
categorization even if the internal workings are flat.

Another interesting aspect of Teragram's categorization technology is
their Rule-Based Categorizer.  Using this system, ``each category
within the directory is associated with a set of rules that describe
documents within that category.''  This may be motivated by a need to
integrate older hand-maintained lists of rules into newer
applications, or it might be meant to address situations like email
categorization in which most documents are indeed best categorized by
simple rules (usually because the sender and receiver have agreed upon
a tagging scheme to mark documents' important properties).  It's not
clear whether Teragram's Rule-Based Categorizer and Automatic
Categorizer can cooperate on a single taxonomy, but they indicate that
the two systems are complementary rather than antithetic.

Teragram also offers separate licensing for many of the tools that
make up its products.  In this sense, it has a strategy similar to one
employed in \aicat's design, in which useful pieces of
functionality created for \aicat\ should be split off into
their own products whenever possible.

\section{Contributions}

During the course of the candidature on which this thesis is based,
the following contributions were accomplished:

\begin{itemize}
\item The \aicat\ framework was designed, implemented, and released
  under an open-source license \cite{cpan}.  The release includes
  documentation and a simple example application using the framework.
\item \naive\ Bayes and Decision Tree categorizers were implemented,
  as well as a mechanism which allows users to use categorizers
  implemented in the Weka Machine Learning system\cite{weka:99}.  A
  simple probabilistic guessing categorizer has also been implemented
  to provide a baseline for experimentation.
\item Contributions from other developers have provided the framework
  with an SVM categorizer.  Collaborative work with other developers
  have provided Rocchio and k-Nearest-Neighbor categorizers.
\item The framework currently has a Document Frequency feature
  selection module implemented.
\item A paper on the design and applicability of the \aicat\ framework
  was published in the proceedings of the Australian Document
  Computing Symposium. \cite{williams:02}
\item A short paper on the use of the \aicat\ framework to categorize
  financial documents was published in the proceedings of the
  Australian Document Computing Symposium. \cite{calvo:02}
\item A paper on the use of \aicat\ to automatically categorize
  mathematics questions is under consideration for publication in the
  proceedings of the conference on Artificial Intelligence in
  Education.  \cite{williams:03}
\item An overview seminar on TC and the design of \aicat\ was given at
  the University of Sydney.  An invited presentation of the same
  seminar was given to the Language Technology group at Macquarie
  University.
\item Tutorials on Machine Learning were presented at the O'Reilly
  2002 Open Source Conference and 2003 Bioinformatics Technology
  Conference (\url{http://conferences.oreilly.com/}).
\item New testing corpora have been assembled in the educational and
  financial domains, and the framework has been evaluated using them
  (see Chapter \ref{Evaluation}).
\end{itemize}


\section{Organization of the Thesis}

Chapter \ref{background-tc} gives a detailed account of the technical
issues in Text Categorization that a TC framework must take into
account.  Chapter \ref{design} discusses design issues in creating the
\aicat\ framework, motivating the design by consideration of the
framework's audience and common usage scenarios, and showing some of
the limitations of the framework's design.  Chapter
\ref{Implementation} is a short discussion of implementation issues.
Chapter \ref{Evaluation} evaluates the framework from several
different perspectives, and Chapter \ref{Conclusion} concludes.
