\chapter{Introduction}

\section{Automatic Text Categorization}
\label{tc-intro}

XXX - need to have more of a summary about why TC and IR are important
(from Rafael: ``start with an overview of why TC and IR are important,
in layman terms.  You need to sell the need for your work, you can not
start with the description of vector models. You can mention
information overload, the need for knowledge management, document and
email filetering and routing, etc'')

The field of automatic Text Categorization is an extremely active area
of current research and application.  It is a multi-disciplinary
field, attracting attention from the Linguistics, Computer Science,
Engineering, and Business communities.  Its applicability is broad,
with many potential uses for large businesses as well as individuals.
A recent survey article from the Association of Computing Machinery
provides a good introduction to the field.\cite{sebastiani:02}

The goal of automatic Text Categorization is to automatically produce specialized
algorithms that can process natural-language documents, assigning zero
or more user-defined labels to them based on their content.  More
formally, given a set of labels (i.e., categories) $\cats = \{c_1, \ldots, c_{|\cats|}\}$ and a set of
previously unseen documents $\docs = \{d_1, d_2, \ldots \}$, a categorizer is a
function that maps from $\docs$ to the set of all subsets of $\cats$.

XXX - create a diagram of a classifier's action on sets

In
some applications, categorizers assign only a single label to each
document, so a categorizer is often a function that maps directly from
$\docs$ to $\cats$.  Often an intermediate function is useful for ``soft'' or 
``rank-based'' categorization, mapping from $\docs \times \cats$ to
the set of real numbers $\mathbb{R}$ in order to assign a score to
each category $c_j$ for each document $d_i$.  The scored categories
may then be presented to a human expert in decreasing order, and the
human may then make the final decision on the document's category
membership.

The standard modern approach to creating new categorizer functions is
to build them using Machine Learning techniques from a set of training
documents $\train$.\cite[p. 2]{sebastiani:02} This is a set of
user-provided, pre-labeled documents that follows a category
distribution similar to the distribution of $\docs$, and whose contents
provide information about what sorts of documents should be mapped to
what sorts of categories.  Algorithms can then be developed that make
generalizations about the relationship between document content and
document category, encoding these generalizations in the learned algorithm.

\section{Object-Oriented Application Frameworks}

An Object-Oriented Application Framework (hereafter referred to simply
as a ``framework'') is a large-scale unit of reusable code in
object-oriented
software development.  Frameworks were developed in response to
situations requiring fine-grained control over the \ldots XXX \ldots

\subsection{Guidelines for designing frameworks}

XXX - needs to be written, distilled from \cite{fayad:99}

 * white-box vs. black-box needs to be defined

\subsection{Design patterns}
\label{patterns}

In order to shed light on the design of complex object-oriented
systems, many researchers and software developers have tried to
standardize language, concepts, and notation for class and object
relationships.  There is as yet no universally accepted terminology
for describing these relationships, but one common practice is to use
so-called ``design patterns'' to provide a baseline grammar for
discussing commonly seen patterns of cooperation in object-oriented
design. \cite[p. 3]{gamma:95} The design patterns do not provide
prescriptions for software design, but rather descriptions of common
practices in common situations.  Most design patterns in
\cite{gamma:95} include discussions of various trade-offs in their
application, indicating that a design pattern is actually a family of
similar solutions to a problem, not one rigid solution.

Design patterns help to illustrate object-oriented software designs
that use \emph{composition} rather than just \emph{inheritance} for
embodying important relationships between objects.  Composition refers
to the practice of multiple independent objects cooperating to achieve
a task, or assembling to form a larger functional unit, while
inheritance refers to the practice of defining a single object's
structure and behavior in terms of both general (``parent'') and
specific (``child'') specifications.  In the language of framework
design and reuse, composition allows for black-box reuse, while pure
inheritance forces white-box reuse.\cite[p. 19]{gamma:95}

The relevance of several design patterns to \aicat\ will be discussed
in detail in Chapter \ref{design}.

\section{Related products}

To discuss the relevance of \aicat\ in the marketplace of Text
Categorization, three related products are examined here.  These
products are by no means the only available products similar to
\aicat, but they provide a reasonable sample of well-known tools for
comparison.

\subsection{Weka}

Weka is an open-source system for Machine Learning originally
developed at the University of Waikato, New Zealand, by Ian H. Witten
and Eibe Frank.\cite{weka:99} Its primary
audience is the international community of academic Machine Learning
researchers, most notably those working with Categorization or
Clustering problems that arise from working with text.  Weka has
undergone at least one major code rewrite; at present it is
implemented as a set of related Java classes with documented internal
interfaces, so it may itself be considered a framework.

Weka is used extensively throughout the academic Text Categorization
community, and as such includes support for many cutting-edge
categorization techniques, including advances in Support Vector
Machines, k-Nearest-Neighbor, \naive\ Bayes, and other categorizers, as
well as several variations of feature selection techniques.  It is
therefore a standard against which the \aicat\ framework can
be measured, as well as a resource which can be leveraged in its
construction.

Despite some similar properties, Weka and \aicat\ differ in
their goals and in many important implementation decisions.  Whereas
Weka specifically targets the academic research community,
\aicat\ aims to support use cases under both
application-building and research situations.
Consequently, Weka will typically keep up with research trends more
closely, but \aicat\ will usually be easier for application
developers to integrate into a real-world situation.

In addition to these differences, another important difference arises
from the different goals in the two projects.  Much of the academic
community is interested in evaluating the correctness and algorithmic
complexity of categorization techniques, whereas most application
developers must also consider resource usage in real-world terms like
time and memory.  In testing, \aicat\ has greatly outperformed
Weka in terms of speed and memory when equivalent algorithms are
compared on identical data sets.  This doesn't reflect an inherent
design flaw in Weka, rather a difference in the kinds of things Weka
developers are likely to spend their time working on.

In order to help facilitate cooperation between the Weka and
\aicat\ communities, as well as leverage existing solutions
inside \aicat, a machine learner class has been created
within \aicat\ that simply passes data through to Weka's
categorizers.  In this way, application developers can easily
experiment with Weka's cutting-edge categorization techniques while
retaining \aicat's application integration advantages.  Any
cross-pollination generated as a result will likely benefit both
projects.  See Section \ref{Adapter-learner} for more information on
the existing bridge to Weka.

Other facilities provided by Weka are not yet offered by
\aicat.  These include visualization tools and several
sophisticated correctness evaluation tools.  Most of these
facilities would make useful additions to \aicat\ if
implemented.

\subsection{Autonomy.com}

Autonomy Corporation (\url{http://www.autonomy.com/}) provides
information services and product licensing to enterprise-level
organizations.  Some of its customers include General Motors,
Ericsson, Sybase, Deutsche Bank, and the United States Department of
Homeland Security.  Its products range broadly over several areas of
Text Processing and Information Retrieval, including categorization,
summarization, and search systems.  The company's web site claims that
their products are ``automatic, language independent, fast, scalable,
and accurate.''  Since the products are proprietary, no independent
verification of these claims has been done in this study, but the
claims do provide a list of attributes this company feels are
important in marketing its products.

The Autonomy web site indicates that its products utilize ``Bayesian
Inference and Claude Shannon's principles of information theory.''
While further details are not provided, this statement would be
consistent with \naive\ Bayes categorization and an Information Gain
feature selection criterion.  However, one must be cautious in making
assessments like this, since there are other ways of employing
Bayesian techniques for categorization that have not been as widely
reported in the research literature as \naive\ Bayes, and Shannon's
information theory pervades many areas of TC and Information
Retrieval, including Decision Tree construction
\cite{quinlan:89,wallace:93} and search relevance ranking
\cite{greiff:00}.

Autonomy suggests that their products can be useful in building
customized portals, Customer Relationship Management (CRM) systems,
enterprise-level search systems and document management tools, and
Human Resources solutions.  These are commonly encountered
applications mentioned (but seldom illustrated) in the TC literature,
and it seems to be generally felt that TC technologies apply broadly
to these application areas.

\subsection{Teragram Corporation}

According to their web site (\url{http://www.teragram.com/}), Teragram
Corporation is a provider of ``fast and stable linguistic
technologies, information search and extraction, knowledge management,
and text processing technologies.''  One of their largest-scale
products is the Teragram Categorizer, an automatic document
categorizer that plays a similar technical role to \aicat.
It cooperates with the Teragram Taxonomy Manager, which provides a
user interface to categories and the documents within each category.

All of Teragram's software products are proprietary, so little
information on implementation is available.  However, product
capabilities and roles can be assessed from the marketing information
given on the web site.  The information presented here has all been
gathered this way.

The Taxonomy Manager is a browser of hierarchical categories, similar
to several on-line directory services like Yahoo
(\url{http://www.yahoo.com/}) or the Open Directory Project
(\url{http://www.dmoz.org/}).  It might therefore be inferred that the
Categorizer is a native hierarchical categorizer, or perhaps that the
categorizer actually flattens the tree structure of the category
hierarchy into a flat list of its leaves, and imposes the tree
structure only afterwards.  Whichever case is true, it must be noted
that the interfaces of the categorizer allow hierarchical
categorization even if the internal workings are flat.

Another interesting aspect of Teragram's categorization technology is
their Rule-Based Categorizer.  Using this system, ``each category
within the directory is associated with a set of rules that describe
documents within that category.''  This may be motivated by a need to
integrate older hand-maintained lists of rules into newer
applications, or it might be meant to address situations like email
categorization in which most documents are indeed best categorized by
simple rules (usually because the sender and receiver have agreed upon
a tagging scheme to mark documents' important properties).  It's not
clear whether Teragram's Rule-Based Categorizer and Automatic
Categorizer can cooperate on a single taxonomy, but they indicate that
the two systems are complementary rather than antithetic.

Teragram also offers separate licensing for many of the tools that
make up its products.  In this sense, it has a strategy similar to one
employed in \aicat's design, in which useful pieces of
functionality created for \aicat\ should be split off into
their own products whenever possible.

\section{Contributions}

During the course of the candidature on which this thesis is based,
the following contributions were accomplished:

\begin{itemize}
\item The \aicat\ framework was designed, implemented, and released
  under an open-source license \cite{cpan}.  The release includes
  documentation and a simple example application using the framework.
\item \naive\ Bayes and Decision Tree categorizers were implemented,
  as well as a mechanism which allows users to use categorizers
  implemented in the Weka Machine Learning system\cite{weka:99}.  A
  simple probabilistic guessing categorizer has also been implemented
  to provide a baseline for experimentation.
\item Contributions from other developers have provided the framework
  with an SVM categorizer.  Collaborative work with other developers
  have provided Rocchio and k-Nearest-Neighbor categorizers.
\item The framework currently has a Document Frequency feature
  selection module implemented.
\item A paper on the design and applicability of the \aicat\ framework
  was published in the proceedings of the Australian Document
  Computing Symposium. \cite{williams:02}
\item A short paper on the use of the \aicat\ framework to categorize
  financial documents was published in the proceedings of the
  Australian Document Computing Symposium. \cite{calvo:02}
\item A paper on the use of \aicat\ to automatically categorize
  mathematics questions is under consideration for publication in the
  proceedings of the conference on Artificial Intelligence in
  Education.  \cite{williams:03}
\item An overview seminar on TC and the design of \aicat\ was given at
  the University of Sydney.  An invited presentation of the same
  seminar was given to the Language Technology group at Macquarie
  University.
\item Tutorials on Machine Learning were presented at the O'Reilly
  2002 Open Source Conference and 2003 Bioinformatics Technology
  Conference (\url{http://conferences.oreilly.com/}).
\item New testing corpora have been assembled in the educational and
  financial domains, and the framework has been evaluated using them
  (see Chapter \ref{Evaluation}).
\end{itemize}


\section{Organization of the Thesis}

Chapter \ref{background-tc} gives a detailed account of the technical
issues in Text Categorization that a TC framework must take into
account.  Chapter \ref{design} discusses design issues in creating the
\aicat\ framework, motivating the design by consideration of the
framework's audience and common usage scenarios, and showing some of
the limitations of the framework's design.  Chapter
\ref{Implementation} is a short discussion of implementation issues.
Chapter \ref{Evaluation} evaluates the framework from several
different perspectives, and Chapter \ref{Conclusion} concludes.
